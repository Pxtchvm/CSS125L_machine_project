{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SRT Subtitle Interpreter\n",
    "## A Complete Language Processing System\n",
    "\n",
    "**Course:** CSS125L - Programming Languages  \n",
    "**Project:** Machine Project - Interpreter Design  \n",
    "**Language:** Python 3.13+\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Introduction\n",
    "\n",
    "## What is an Interpreter?\n",
    "\n",
    "An **interpreter** is a program that directly executes instructions written in a programming or scripting language without requiring them to be compiled into machine code first. Unlike compilers that translate source code to executable binaries, interpreters process and execute code line-by-line or statement-by-statement.\n",
    "\n",
    "**Compiler vs Interpreter:**\n",
    "- **Compiler**: Source Code → Compilation → Machine Code → Execution\n",
    "- **Interpreter**: Source Code → Direct Execution (with optional parsing/translation steps)\n",
    "\n",
    "## Why Subtitles Need Interpreters\n",
    "\n",
    "Subtitle files (.srt, .vtt, .ass) contain:\n",
    "1. **Timing information** - When each subtitle should appear and disappear\n",
    "2. **Text content** - The actual subtitle text\n",
    "3. **Formatting metadata** - Styling, positioning, colors\n",
    "\n",
    "An interpreter is needed to:\n",
    "- **Parse** the subtitle format (lexical and syntax analysis)\n",
    "- **Validate** timing constraints (no overlaps, sequential ordering)\n",
    "- **Execute** time-synchronized display\n",
    "- **Transform** content (translation, formatting, export)\n",
    "\n",
    "## Real-World Applications\n",
    "\n",
    "SRT subtitle processing is used in:\n",
    "- **Video Players** (VLC, MPC-HC) - Real-time subtitle rendering\n",
    "- **Streaming Services** (Netflix, YouTube) - Multi-language subtitle management\n",
    "- **Subtitle Editors** (Aegisub, Subtitle Edit) - Creation and modification\n",
    "- **Translation Services** - Automated subtitle localization\n",
    "- **Accessibility Tools** - Closed captioning for hearing impaired\n",
    "\n",
    "## Our Interpreter System\n",
    "\n",
    "This project implements a complete SRT subtitle interpreter with:\n",
    "\n",
    "**Core Features:**\n",
    "- Lexical analysis (tokenization)\n",
    "- Syntax parsing with validation\n",
    "- Abstract Syntax Tree (AST) construction\n",
    "- Time-synchronized execution (3 modes)\n",
    "\n",
    "**Advanced Features:**\n",
    "- Multi-language translation (5 languages)\n",
    "- Statistics calculation\n",
    "- Export functionality (text and SRT)\n",
    "- ANSI formatting for HTML tags\n",
    "\n",
    "**Design Goals:**\n",
    "- Educational clarity over optimization\n",
    "- Comprehensive error handling\n",
    "- Modular, testable architecture\n",
    "- Real-world applicability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Input Language Description\n",
    "\n",
    "## SRT Format Specification\n",
    "\n",
    "The SubRip Text (.srt) format is a simple subtitle format with the following structure:\n",
    "\n",
    "```\n",
    "<index>\n",
    "<start_timestamp> --> <end_timestamp>\n",
    "<subtitle_text_line_1>\n",
    "[<subtitle_text_line_2>...]\n",
    "<blank_line>\n",
    "```\n",
    "\n",
    "**Components:**\n",
    "1. **Index**: Sequential number (1, 2, 3, ...)\n",
    "2. **Timestamp line**: `HH:MM:SS,mmm --> HH:MM:SS,mmm`\n",
    "   - Hours: 00-99\n",
    "   - Minutes: 00-59\n",
    "   - Seconds: 00-59\n",
    "   - Milliseconds: 000-999\n",
    "3. **Text lines**: One or more lines of subtitle text\n",
    "4. **Blank line**: Separator between subtitle blocks\n",
    "\n",
    "**Optional Features:**\n",
    "- HTML-like formatting tags: `<i>`, `<b>`, `<u>`, `<font color=\"#RRGGBB\">`\n",
    "- Multi-line text content\n",
    "- UTF-8 encoding for international characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VALID SRT FILE: examples/valid_basic.srt\n",
      "============================================================\n",
      "1\n",
      "00:00:01,000 --> 00:00:03,000\n",
      "Hello world!\n",
      "\n",
      "2\n",
      "00:00:04,000 --> 00:00:06,000\n",
      "This is a test.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Valid SRT File\n",
    "with open('examples/valid_basic.srt', 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "    \n",
    "print(\"=\" * 60)\n",
    "print(\"VALID SRT FILE: examples/valid_basic.srt\")\n",
    "print(\"=\" * 60)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INVALID: examples/invalid_missing_index.srt\n",
      "Error: Missing index number\n",
      "============================================================\n",
      "1\n",
      "00:00:01,000 --> 00:00:03,000\n",
      "First subtitle is fine.\n",
      "\n",
      "00:00:04,000 --> 00:00:06,000\n",
      "This subtitle is missing its index!\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "INVALID: examples/invalid_timestamp_order.srt\n",
      "Error: Start time after end time\n",
      "============================================================\n",
      "1\n",
      "00:00:01,000 --> 00:00:03,000\n",
      "First subtitle is fine.\n",
      "\n",
      "2\n",
      "00:00:08,000 --> 00:00:05,000\n",
      "This subtitle has start time AFTER end time!\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Invalid SRT Files\n",
    "invalid_files = [\n",
    "    ('examples/invalid_missing_index.srt', 'Missing index number'),\n",
    "    ('examples/invalid_timestamp_order.srt', 'Start time after end time')\n",
    "]\n",
    "\n",
    "for filepath, description in invalid_files:\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"INVALID: {filepath}\")\n",
    "    print(f\"Error: {description}\")\n",
    "    print(\"=\" * 60)\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        print(f.read())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Types and Grammar Rules\n",
    "\n",
    "### Token Types\n",
    "\n",
    "Our lexer recognizes the following token types:\n",
    "\n",
    "1. **INDEX**: `^\\d+$` - Sequential subtitle number\n",
    "2. **TIMESTAMP**: `\\d{2}:\\d{2}:\\d{2},\\d{3}` - Time in HH:MM:SS,mmm format\n",
    "3. **ARROW**: `-->` - Separator between start and end timestamps\n",
    "4. **TEXT**: Any non-empty line that isn't index/timestamp/arrow\n",
    "5. **FORMATTING_TAG**: HTML-like tags (`<i>`, `</i>`, `<b>`, etc.)\n",
    "6. **NEWLINE**: `\\n` - Line break\n",
    "7. **BLANK_LINE**: `\\n\\n` - Subtitle block separator\n",
    "8. **EOF**: End of file marker\n",
    "\n",
    "### Formal Grammar (EBNF)\n",
    "\n",
    "```ebnf\n",
    "subtitle_file    = subtitle_block+ EOF\n",
    "subtitle_block   = INDEX NEWLINE timestamp_line NEWLINE text_lines BLANK_LINE\n",
    "timestamp_line   = TIMESTAMP ARROW TIMESTAMP\n",
    "text_lines       = TEXT (NEWLINE TEXT)*\n",
    "```\n",
    "\n",
    "### Validation Rules\n",
    "\n",
    "1. **Sequential indexes**: Must be 1, 2, 3, ... (no gaps)\n",
    "2. **Timestamp validity**: MM ≤ 59, SS ≤ 59, mmm ≤ 999\n",
    "3. **Time ordering**: start_time < end_time for each subtitle\n",
    "4. **Non-empty text**: At least one text line required\n",
    "5. **Block structure**: Proper blank line separation\n",
    "\n",
    "## Design Rationale\n",
    "\n",
    "**Why .srt is ideal for an interpreter project:**\n",
    "\n",
    "1. **Clear lexical structure** - Easy to tokenize with regex patterns\n",
    "2. **Simple grammar** - Straightforward parsing rules\n",
    "3. **Rich validation opportunities** - Temporal, sequential, structural constraints\n",
    "4. **Real-world relevance** - Widely used format\n",
    "5. **Extension potential** - Translation, formatting, export features\n",
    "6. **Educational value** - Demonstrates all interpreter phases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: System Design\n",
    "\n",
    "## Technology Stack\n",
    "\n",
    "### Python Version\n",
    "- **Python 3.13+** (latest features, improved type system)\n",
    "\n",
    "### Built-in Libraries\n",
    "\n",
    "- **`re`** - Regular expressions for pattern matching and tokenization\n",
    "- **`time`** - Time simulation for real-time subtitle execution\n",
    "- **`sys`** - System operations and exit codes\n",
    "- **`pathlib`** - Modern file path operations\n",
    "- **`typing`** - Type hints for code clarity and IDE support\n",
    "- **`dataclasses`** - Immutable AST node structures with `frozen=True`\n",
    "- **`hashlib`** - MD5 hashing for translation cache keys\n",
    "- **`argparse`** - Command-line interface argument parsing\n",
    "\n",
    "### Third-party Libraries\n",
    "\n",
    "- **`deep-translator`** - Multi-language translation via Google Translate API\n",
    "  - Supports 100+ languages\n",
    "  - Free tier available\n",
    "  - Used for Filipino, Korean, Chinese, Japanese translation\n",
    "\n",
    "## Design Principles\n",
    "\n",
    "### 1. Pipeline Architecture (Separation of Concerns)\n",
    "```\n",
    "Input → Lexer → Parser → Translator → Executor → Output\n",
    "```\n",
    "Each component has a single, well-defined responsibility.\n",
    "\n",
    "### 2. Immutable AST Nodes\n",
    "- Use `@dataclass(frozen=True)` for TimeStamp and SubtitleEntry\n",
    "- Prevents accidental modification\n",
    "- Thread-safe by design\n",
    "- Easier debugging (no unexpected state changes)\n",
    "\n",
    "### 3. Comprehensive Error Handling\n",
    "- Custom exception hierarchy: `LexerError`, `ParserError`, `TranslationError`, etc.\n",
    "- Descriptive error messages with context\n",
    "- Early validation at each stage\n",
    "- Graceful degradation (e.g., translation fallback to English)\n",
    "\n",
    "### 4. Translation Caching Strategy\n",
    "- File-based cache using MD5 hash of source content\n",
    "- Cache key: `{md5_hash}_{source_lang}_{target_lang}.json`\n",
    "- Instant retrieval for previously translated files\n",
    "- Offline capability after first translation\n",
    "- Batch translation to reduce API calls\n",
    "\n",
    "### 5. Type Safety\n",
    "- Full type hints throughout codebase\n",
    "- IDE autocomplete support\n",
    "- Early error detection\n",
    "- Self-documenting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.13.7 (main, Sep 18 2025, 19:47:49) [Clang 20.1.4 ]\n",
      "Python executable: /home/pxtchvm/projects/CSS125L_machine_project/.venv/bin/python\n",
      "\n",
      "Library availability:\n",
      "  re                   ✓ Available\n",
      "  time                 ✓ Available\n",
      "  pathlib              ✓ Available\n",
      "  typing               ✓ Available\n",
      "  dataclasses          ✓ Available\n",
      "  hashlib              ✓ Available\n",
      "  argparse             ✓ Available\n",
      "  deep_translator      ✓ Available\n"
     ]
    }
   ],
   "source": [
    "# Verify Python version and imports\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print()\n",
    "\n",
    "# Verify all required libraries\n",
    "libraries = [\n",
    "    're', 'time', 'pathlib', 'typing', 'dataclasses', \n",
    "    'hashlib', 'argparse', 'deep_translator'\n",
    "]\n",
    "\n",
    "print(\"Library availability:\")\n",
    "for lib in libraries:\n",
    "    try:\n",
    "        __import__(lib)\n",
    "        print(f\"  {lib:20} ✓ Available\")\n",
    "    except ImportError:\n",
    "        print(f\"  {lib:20} ✗ Missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Architecture Overview\n",
    "\n",
    "## Data Flow Diagram\n",
    "\n",
    "The following diagram illustrates the complete data flow through the interpreter pipeline:\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Input .srt File] --> B[LEXER<br/>Tokenization<br/>Pattern Recognition]\n",
    "    B -->|Token Stream<br/>INDEX, NEWLINE, TIMESTAMP, ARROW, ...| C[PARSER<br/>Syntax Analysis<br/>AST Construction]\n",
    "    C -->|AST SubtitleEntry objects<br/>Entry1, Entry2, Entry3, ...| D[TRANSLATOR<br/>Semantic Transformation<br/>Multi-language, Optional]\n",
    "    D -->|Translated AST| E{Output<br/>Selection}\n",
    "    \n",
    "    E -->|Execute| F[EXECUTOR]\n",
    "    E -->|Analyze| G[STATS]\n",
    "    E -->|Export| H[EXPORT]\n",
    "    E -->|Format| I[FORMATTER]\n",
    "    \n",
    "    F --> J[Display]\n",
    "    G --> K[Statistics]\n",
    "    H --> L[Files]\n",
    "    I --> M[ANSI Output]\n",
    "```\n",
    "\n",
    "## Component Descriptions\n",
    "\n",
    "### 1. Lexer (`src/lexer.py`)\n",
    "\n",
    "**Input:** Raw text string (file content)\n",
    "\n",
    "**Output:** List of Token objects\n",
    "\n",
    "**Responsibility:**\n",
    "- Pattern recognition using regular expressions\n",
    "- Character stream → token stream conversion\n",
    "- Initial format validation\n",
    "\n",
    "**Key Patterns:**\n",
    "- Timestamp: `r'\\d{2}:\\d{2}:\\d{2},\\d{3}'`\n",
    "- Arrow: `r'-->'`\n",
    "- Index: `r'^\\d+$'` (on its own line)\n",
    "\n",
    "**Error Detection:**\n",
    "- Invalid characters in timestamp\n",
    "- Malformed token sequences\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Parser (`src/parser.py`, `src/ast_nodes.py`)\n",
    "\n",
    "**Input:** Token stream from Lexer\n",
    "\n",
    "**Output:** List of SubtitleEntry AST nodes\n",
    "\n",
    "**Responsibility:**\n",
    "- Syntax validation (grammar enforcement)\n",
    "- Semantic checks (time ordering, index sequence)\n",
    "- AST construction with immutable nodes\n",
    "\n",
    "**Validations Performed:**\n",
    "1. Sequential indexes (1, 2, 3, ...)\n",
    "2. Valid timestamp ranges (MM/SS ≤ 59, mmm ≤ 999)\n",
    "3. Start time < end time\n",
    "4. Non-empty text content\n",
    "5. Proper block structure\n",
    "\n",
    "**AST Nodes:**\n",
    "- `TimeStamp`: Immutable time representation\n",
    "- `SubtitleEntry`: Complete subtitle with validation\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Translator (`src/translator.py`)\n",
    "\n",
    "**Input:** AST + target language\n",
    "\n",
    "**Output:** Translated AST (new SubtitleEntry objects with translated text)\n",
    "\n",
    "**Responsibility:**\n",
    "- Multi-language translation via Google Translate\n",
    "- File-based caching for performance\n",
    "- Batch processing to reduce API calls\n",
    "\n",
    "**Supported Languages:**\n",
    "- English (passthrough, no translation)\n",
    "- Filipino (Tagalog)\n",
    "- Korean\n",
    "- Chinese (Simplified)\n",
    "- Japanese\n",
    "\n",
    "**Caching Strategy:**\n",
    "- MD5 hash of source content as cache key\n",
    "- JSON files stored in `.srt_cache/` directory\n",
    "- Instant retrieval for repeated translations\n",
    "- Fallback to English if translation fails\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Executor (`src/executor.py`)\n",
    "\n",
    "**Input:** AST + execution mode + formatting options\n",
    "\n",
    "**Output:** Console display with timing\n",
    "\n",
    "**Execution Modes:**\n",
    "\n",
    "1. **Sequential**: Display each subtitle with brief pause (0.5s)\n",
    "   - Fast demonstration mode\n",
    "   - No timing simulation\n",
    "\n",
    "2. **Real-time**: Display at actual timestamps\n",
    "   - Faithful to original timing\n",
    "   - Takes actual duration to complete\n",
    "\n",
    "3. **Accelerated**: Display with speed multiplier\n",
    "   - Configurable speed (e.g., 5x, 10x)\n",
    "   - Maintains timing relationships\n",
    "\n",
    "**Output Format:**\n",
    "```\n",
    "[HH:MM:SS.mmm] DISPLAY: \"subtitle text\"\n",
    "[HH:MM:SS.mmm] CLEAR\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Extensions\n",
    "\n",
    "#### Statistics (`src/stats.py`)\n",
    "- Total entries and duration\n",
    "- Average subtitle duration and text length\n",
    "- Longest/shortest by duration and text length\n",
    "\n",
    "#### Export (`src/export.py`)\n",
    "- **Text export**: Plain, numbered, or separated formats\n",
    "- **SRT export**: Complete translated subtitle file\n",
    "\n",
    "#### Formatter (`src/formatter.py`)\n",
    "- HTML to ANSI escape code conversion\n",
    "- Tag support: `<i>`, `<b>`, `<u>`, `<font color>`\n",
    "- 24-bit RGB color support\n",
    "\n",
    "## Error Handling Strategy\n",
    "\n",
    "### Exception Hierarchy\n",
    "```python\n",
    "Exception\n",
    "  ├── LexerError        # Tokenization failures\n",
    "  ├── ParserError       # Syntax/semantic violations\n",
    "  ├── TranslationError  # Translation failures\n",
    "  ├── ExecutorError     # Execution failures\n",
    "  ├── StatisticsError   # Statistics calculation errors\n",
    "  ├── ExportError       # Export operation failures\n",
    "  └── FormatterError    # Formatting conversion errors\n",
    "```\n",
    "\n",
    "### Error Messages\n",
    "- Include specific context (line number, token, expected vs actual)\n",
    "- Actionable suggestions when possible\n",
    "- Clear indication of error location\n",
    "\n",
    "### Graceful Degradation\n",
    "- Translation fallback to English if API fails\n",
    "- Continue execution after non-critical errors\n",
    "- User-friendly error reporting\n",
    "\n",
    "## Design Decisions & Justifications\n",
    "\n",
    "### Why Pipeline Architecture?\n",
    "- **Modularity**: Each component can be developed/tested independently\n",
    "- **Maintainability**: Changes isolated to specific components\n",
    "- **Extensibility**: Easy to add new features (e.g., new output formats)\n",
    "- **Educational**: Clear demonstration of interpreter phases\n",
    "\n",
    "### Why Frozen Dataclasses for AST?\n",
    "- **Immutability**: Prevents accidental modifications\n",
    "- **Thread safety**: Safe for concurrent operations\n",
    "- **Debugging**: No unexpected state changes\n",
    "- **Clarity**: Explicit about data flow\n",
    "\n",
    "### Why File-based Caching?\n",
    "- **Performance**: Instant retrieval for repeated translations\n",
    "- **Offline capability**: Works without internet after first translation\n",
    "- **Persistence**: Cache survives program restarts\n",
    "- **Transparency**: User can inspect/clear cache files\n",
    "\n",
    "### Why Batch Translation?\n",
    "- **API efficiency**: Fewer API calls = faster execution\n",
    "- **Rate limiting**: Reduces chance of hitting API limits\n",
    "- **Atomicity**: All subtitles translated together\n",
    "- **Consistency**: Same translation context for all entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Implementation Details\n",
    "\n",
    "This section demonstrates the core implementation of each component with code examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Lexer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LEXER: Token Types\n",
      "============================================================\n",
      "\n",
      "Available token types:\n",
      "  - TOKEN_ARROW: 'ARROW'\n",
      "  - TOKEN_BLANK_LINE: 'BLANK_LINE'\n",
      "  - TOKEN_EOF: 'EOF'\n",
      "  - TOKEN_FORMATTING_TAG: 'FORMATTING_TAG'\n",
      "  - TOKEN_INDEX: 'INDEX'\n",
      "  - TOKEN_NEWLINE: 'NEWLINE'\n",
      "  - TOKEN_TEXT: 'TEXT'\n",
      "  - TOKEN_TIMESTAMP: 'TIMESTAMP'\n",
      "\n",
      "============================================================\n",
      "LEXER: Token Class Structure\n",
      "============================================================\n",
      "@dataclass\n",
      "class Token:\n",
      "    \"\"\"Represents a single lexical token.\"\"\"\n",
      "    type: str\n",
      "    value: str\n",
      "    line_number: int = 0\n",
      "\n",
      "    def __repr__(self):\n",
      "        return f\"Token({self.type}, {self.value!r})\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show Lexer token types and core logic\n",
    "from src.lexer import Lexer, Token\n",
    "import inspect\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LEXER: Token Types\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nAvailable token types:\")\n",
    "\n",
    "# Get all token type constants from lexer module\n",
    "import src.lexer as lexer_module\n",
    "token_types = [\n",
    "    (name, getattr(lexer_module, name)) \n",
    "    for name in dir(lexer_module) \n",
    "    if name.startswith('TOKEN_')\n",
    "]\n",
    "\n",
    "for name, value in token_types:\n",
    "    print(f\"  - {name}: {value!r}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LEXER: Token Class Structure\")\n",
    "print(\"=\"*60)\n",
    "print(inspect.getsource(Token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LEXER DEMONSTRATION\n",
      "============================================================\n",
      "\n",
      "Input SRT content:\n",
      "1\n",
      "00:00:01,000 --> 00:00:03,000\n",
      "Hello world!\n",
      "\n",
      "2\n",
      "00:00:04,000 --> 00:00:06,000\n",
      "This is a <i>test</i>.\n",
      "\n",
      "\n",
      "\n",
      "Tokenization result:\n",
      "------------------------------------------------------------\n",
      "  1. Token(INDEX, '1')\n",
      "  2. Token(NEWLINE, '\\n')\n",
      "  3. Token(TIMESTAMP, '00:00:01,000')\n",
      "  4. Token(ARROW, '-->')\n",
      "  5. Token(TIMESTAMP, '00:00:03,000')\n",
      "  6. Token(NEWLINE, '\\n')\n",
      "  7. Token(TEXT, 'Hello world!')\n",
      "  8. Token(NEWLINE, '\\n')\n",
      "  9. Token(BLANK_LINE, '')\n",
      " 10. Token(INDEX, '2')\n",
      " 11. Token(NEWLINE, '\\n')\n",
      " 12. Token(TIMESTAMP, '00:00:04,000')\n",
      " 13. Token(ARROW, '-->')\n",
      " 14. Token(TIMESTAMP, '00:00:06,000')\n",
      " 15. Token(NEWLINE, '\\n')\n",
      " 16. Token(TEXT, 'This is a <i>test</i>.')\n",
      " 17. Token(NEWLINE, '\\n')\n",
      " 18. Token(BLANK_LINE, '')\n",
      " 19. Token(BLANK_LINE, '')\n",
      " 20. Token(EOF, '')\n",
      "------------------------------------------------------------\n",
      "Total tokens generated: 20\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate lexer tokenization\n",
    "sample_srt = \"\"\"1\n",
    "00:00:01,000 --> 00:00:03,000\n",
    "Hello world!\n",
    "\n",
    "2\n",
    "00:00:04,000 --> 00:00:06,000\n",
    "This is a <i>test</i>.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LEXER DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nInput SRT content:\")\n",
    "print(sample_srt)\n",
    "print(\"\\nTokenization result:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "lexer = Lexer()\n",
    "tokens = lexer.tokenize(sample_srt)\n",
    "\n",
    "for i, token in enumerate(tokens, 1):\n",
    "    print(f\"{i:3}. {token}\")\n",
    "\n",
    "print(\"-\"*60)\n",
    "print(f\"Total tokens generated: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Parser Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "AST NODE: TimeStamp\n",
      "============================================================\n",
      "@dataclass\n",
      "class TimeStamp:\n",
      "    \"\"\"\n",
      "    Represents a timestamp in SRT format (HH:MM:SS,mmm).\n",
      "\n",
      "    Attributes:\n",
      "        hours: Hours (0-99)\n",
      "        minutes: Minutes (0-59)\n",
      "        seconds: Seconds (0-59)\n",
      "        milliseconds: Milliseconds (0-999)\n",
      "    \"\"\"\n",
      "    hours: int\n",
      "    minutes: int\n",
      "    seconds: int\n",
      "    milliseconds: int\n",
      "\n",
      "    @classmethod\n",
      "    def from_string(cls, timestamp_str: str) -> 'TimeStamp':\n",
      "        \"\"\"\n",
      "        Parse a timestamp string into a TimeStamp object.\n",
      "\n",
      "        Args:\n",
      "            timestamp_str: Timestamp in format HH:MM:SS,mmm\n",
      "\n",
      "        Returns:\n",
      "            TimeStamp object\n",
      "\n",
      "        Raises:\n",
      "            ValueError: If timestamp format is invalid or values out of range\n",
      "        \"\"\"\n",
      "        try:\n",
      "            # Split by comma to separate seconds and milliseconds\n",
      "            time_part, ms_part = timestamp_str.split(',')\n",
      "\n",
      "            # Split time part by colon\n",
      "            hours_str, minutes_str, seconds_str = time_part.split(':')\n",
      "\n",
      "            hours = int(hours_str)\n",
      "            minutes = int(minutes_str)\n",
      "            seconds = int(seconds_str)\n",
      "            milliseconds = int(ms_part)\n",
      "\n",
      "            # Validate ranges\n",
      "            if not (0 <= hours <= 99):\n",
      "                raise ValueError(f\"Hours must be between 0 and 99, got {hours}\")\n",
      "            if not (0 <= minutes <= 59):\n",
      "                raise ValueError(f\"Minutes must be between 0 and 59, got {minutes}\")\n",
      "            if not (0 <= seconds <= 59):\n",
      "                raise ValueError(f\"Seconds must be between 0 and 59, got {seconds}\")\n",
      "            if not (0 <= milliseconds <= 999):\n",
      "                raise ValueError(f\"Milliseconds must be between 0 and 999, got {milliseconds}\")\n",
      "\n",
      "            return cls(hours, minutes, seconds, milliseconds)\n",
      "\n",
      "        except (ValueError, AttributeError) as e:\n",
      "            raise ValueError(f\"Invalid timestamp format '{timestamp_str}': {e}\")\n",
      "\n",
      "    def to_milliseconds(self) -> int:\n",
      "        \"\"\"\n",
      "        Convert timestamp to total milliseconds.\n",
      "\n",
      "        Returns:\n",
      "            Total milliseconds\n",
      "        \"\"\"\n",
      "        return (\n",
      "            self.hours * 3600000 +\n",
      "            self.minutes * 60000 +\n",
      "            self.seconds * 1000 +\n",
      "            self.milliseconds\n",
      "        )\n",
      "\n",
      "    def __lt__(self, other: 'TimeStamp') -> bool:\n",
      "        \"\"\"Compare timestamps for ordering.\"\"\"\n",
      "        return self.to_milliseconds() < other.to_milliseconds()\n",
      "\n",
      "    def __le__(self, other: 'TimeStamp') -> bool:\n",
      "        \"\"\"Compare timestamps for ordering.\"\"\"\n",
      "        return self.to_milliseconds() <= other.to_milliseconds()\n",
      "\n",
      "    def __gt__(self, other: 'TimeStamp') -> bool:\n",
      "        \"\"\"Compare timestamps for ordering.\"\"\"\n",
      "        return self.to_milliseconds() > other.to_milliseconds()\n",
      "\n",
      "    def __ge__(self, other: 'TimeStamp') -> bool:\n",
      "        \"\"\"Compare timestamps for ordering.\"\"\"\n",
      "        return self.to_milliseconds() >= other.to_milliseconds()\n",
      "\n",
      "    def __str__(self) -> str:\n",
      "        \"\"\"String representation in SRT format.\"\"\"\n",
      "        return f\"{self.hours:02d}:{self.minutes:02d}:{self.seconds:02d},{self.milliseconds:03d}\"\n",
      "\n",
      "\n",
      "============================================================\n",
      "AST NODE: SubtitleEntry\n",
      "============================================================\n",
      "@dataclass\n",
      "class SubtitleEntry:\n",
      "    \"\"\"\n",
      "    Represents a single subtitle entry in an SRT file.\n",
      "\n",
      "    Attributes:\n",
      "        index: Sequential subtitle number (1, 2, 3...)\n",
      "        start_time: When subtitle should appear\n",
      "        end_time: When subtitle should disappear\n",
      "        text: List of text lines for this subtitle\n",
      "        formatting: Optional formatting tags present in the text\n",
      "    \"\"\"\n",
      "    index: int\n",
      "    start_time: TimeStamp\n",
      "    end_time: TimeStamp\n",
      "    text: List[str]\n",
      "    formatting: Optional[List[str]] = None\n",
      "\n",
      "    def validate(self) -> None:\n",
      "        \"\"\"\n",
      "        Validate the subtitle entry.\n",
      "\n",
      "        Raises:\n",
      "            ValueError: If validation fails\n",
      "        \"\"\"\n",
      "        # Check index is positive\n",
      "        if self.index < 1:\n",
      "            raise ValueError(f\"Index must be positive, got {self.index}\")\n",
      "\n",
      "        # Check start time is before end time\n",
      "        if self.start_time >= self.end_time:\n",
      "            raise ValueError(\n",
      "                f\"Start time ({self.start_time}) must be before end time ({self.end_time})\"\n",
      "            )\n",
      "\n",
      "        # Check text is not empty\n",
      "        if not self.text or all(not line.strip() for line in self.text):\n",
      "            raise ValueError(f\"Subtitle {self.index} has no text content\")\n",
      "\n",
      "    def get_text(self) -> str:\n",
      "        \"\"\"\n",
      "        Get the full text content as a single string.\n",
      "\n",
      "        Returns:\n",
      "            All text lines joined with newlines\n",
      "        \"\"\"\n",
      "        return '\\n'.join(self.text)\n",
      "\n",
      "    def __str__(self) -> str:\n",
      "        \"\"\"String representation showing key information.\"\"\"\n",
      "        text_preview = self.get_text()[:50]\n",
      "        if len(self.get_text()) > 50:\n",
      "            text_preview += \"...\"\n",
      "        return f\"SubtitleEntry({self.index}: {self.start_time} → {self.end_time}, '{text_preview}')\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show AST node structures\n",
    "from src.ast_nodes import TimeStamp, SubtitleEntry\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"AST NODE: TimeStamp\")\n",
    "print(\"=\"*60)\n",
    "print(inspect.getsource(TimeStamp))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AST NODE: SubtitleEntry\")\n",
    "print(\"=\"*60)\n",
    "print(inspect.getsource(SubtitleEntry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PARSER DEMONSTRATION\n",
      "============================================================\n",
      "\n",
      "Parsing the tokenized input...\n",
      "\n",
      "Parsed 2 subtitle entries:\n",
      "\n",
      "Entry 1:\n",
      "  Start time: 00:00:01,000\n",
      "  End time:   00:00:03,000\n",
      "  Duration:   2.00s\n",
      "  Text:       'Hello world!'\n",
      "  Lines:      ['Hello world!']\n",
      "\n",
      "Entry 2:\n",
      "  Start time: 00:00:04,000\n",
      "  End time:   00:00:06,000\n",
      "  Duration:   2.00s\n",
      "  Text:       'This is a <i>test</i>.'\n",
      "  Lines:      ['This is a <i>test</i>.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate parser with AST construction\n",
    "from src.parser import Parser\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PARSER DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nParsing the tokenized input...\\n\")\n",
    "\n",
    "parser = Parser(tokens)\n",
    "entries = parser.parse()\n",
    "\n",
    "print(f\"Parsed {len(entries)} subtitle entries:\\n\")\n",
    "\n",
    "for entry in entries:\n",
    "    print(f\"Entry {entry.index}:\")\n",
    "    print(f\"  Start time: {entry.start_time}\")\n",
    "    print(f\"  End time:   {entry.end_time}\")\n",
    "    print(f\"  Duration:   {(entry.end_time.to_milliseconds() - entry.start_time.to_milliseconds()) / 1000:.2f}s\")\n",
    "    print(f\"  Text:       {entry.get_text()!r}\")\n",
    "    print(f\"  Lines:      {entry.text}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Translator Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRANSLATOR DEMONSTRATION\n",
      "============================================================\n",
      "\n",
      "Original entries (English):\n",
      "  1. Hello world!\n",
      "  2. This is a <i>test</i>.\n",
      "\n",
      "Translating to Filipino...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Translating: 100%|██████████| 2/2 [00:01<00:00,  1.18subtitle/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translated entries (Filipino):\n",
      "  1. Hello World!\n",
      "  2. Ito ay isang <i> pagsubok </i>.\n",
      "\n",
      "Translating to Korean...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Translating: 100%|██████████| 2/2 [00:01<00:00,  1.52subtitle/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translated entries (Korean):\n",
      "  1. 안녕하세요!\n",
      "  2. 이것은 <i>테스트</i>입니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate translation with caching\n",
    "from src.translator import Translator\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRANSLATOR DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nOriginal entries (English):\")\n",
    "for entry in entries:\n",
    "    print(f\"  {entry.index}. {entry.get_text()}\")\n",
    "\n",
    "# Translate to Filipino\n",
    "print(\"\\nTranslating to Filipino...\")\n",
    "translator_fil = Translator('filipino')\n",
    "translated_fil = translator_fil.translate_entries(entries, file_content=sample_srt)\n",
    "\n",
    "print(\"\\nTranslated entries (Filipino):\")\n",
    "for entry in translated_fil:\n",
    "    print(f\"  {entry.index}. {entry.get_text()}\")\n",
    "\n",
    "# Translate to Korean\n",
    "print(\"\\nTranslating to Korean...\")\n",
    "translator_ko = Translator('korean')\n",
    "translated_ko = translator_ko.translate_entries(entries, file_content=sample_srt)\n",
    "\n",
    "print(\"\\nTranslated entries (Korean):\")\n",
    "for entry in translated_ko:\n",
    "    print(f\"  {entry.index}. {entry.get_text()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Executor Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXECUTOR DEMONSTRATION: Sequential Mode\n",
      "============================================================\n",
      "\n",
      "[00:00:01.000] DISPLAY: \"Hello world!\"\n",
      "[00:00:03.000] CLEAR\n",
      "[00:00:04.000] DISPLAY: \"This is a test.\"\n",
      "[00:00:06.000] CLEAR\n",
      "\n",
      "Execution complete!\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate sequential execution mode\n",
    "from src.executor import Executor\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EXECUTOR DEMONSTRATION: Sequential Mode\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "executor = Executor()\n",
    "executor.execute(entries, mode=\"sequential\", enable_formatting=False)\n",
    "\n",
    "print()\n",
    "print(\"Execution complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXECUTOR DEMONSTRATION: Accelerated Mode (10x speed)\n",
      "============================================================\n",
      "\n",
      "[00:00:01.000] DISPLAY: \"Hello world!\"\n",
      "[00:00:03.000] CLEAR\n",
      "[00:00:04.000] DISPLAY: \"This is a test.\"\n",
      "[00:00:06.000] CLEAR\n",
      "\n",
      "Accelerated execution complete!\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate accelerated execution mode\n",
    "print(\"=\"*60)\n",
    "print(\"EXECUTOR DEMONSTRATION: Accelerated Mode (10x speed)\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "executor_accel = Executor()\n",
    "executor_accel.execute(entries, mode=\"accelerated\", speed_factor=10.0, enable_formatting=False)\n",
    "\n",
    "print()\n",
    "print(\"Accelerated execution complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Error Handling Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ERROR HANDLING DEMONSTRATION\n",
      "============================================================\n",
      "\n",
      "------------------------------------------------------------\n",
      "Testing: examples/invalid_timestamp_order.srt\n",
      "Expected: Start time after end time\n",
      "------------------------------------------------------------\n",
      "File content:\n",
      "1\n",
      "00:00:01,000 --> 00:00:03,000\n",
      "First subtitle is fine.\n",
      "\n",
      "2\n",
      "00:00:08,000 --> 00:00:05,000\n",
      "This subtitle has start time AFTER end time!\n",
      "\n",
      "\n",
      "\n",
      "✓ Error caught successfully:\n",
      "  Type: ParserError\n",
      "  Message: Start time (00:00:08,000) must be before end time (00:00:05,000)\n",
      "\n",
      "------------------------------------------------------------\n",
      "Testing: examples/invalid_missing_index.srt\n",
      "Expected: Missing index\n",
      "------------------------------------------------------------\n",
      "File content:\n",
      "1\n",
      "00:00:01,000 --> 00:00:03,000\n",
      "First subtitle is fine.\n",
      "\n",
      "00:00:04,000 --> 00:00:06,000\n",
      "This subtitle is missing its index!\n",
      "\n",
      "\n",
      "\n",
      "✓ Error caught successfully:\n",
      "  Type: ParserError\n",
      "  Message: Expected subtitle index 2, but got TIMESTAMP\n",
      "\n",
      "------------------------------------------------------------\n",
      "Testing: examples/invalid_malformed_time.srt\n",
      "Expected: Invalid timestamp format\n",
      "------------------------------------------------------------\n",
      "File content:\n",
      "1\n",
      "00:00:01,000 --> 00:00:03,000\n",
      "First subtitle is fine.\n",
      "\n",
      "2\n",
      "00:00:04,000 --> 00:99:99,999\n",
      "This subtitle has invalid timestamp ranges (minutes and seconds > 59)!\n",
      "\n",
      "\n",
      "\n",
      "✓ Error caught successfully:\n",
      "  Type: ParserError\n",
      "  Message: Invalid end timestamp: Invalid timestamp format '00:99:99,999': Minutes must be between 0 and 59, got 99\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate error handling with invalid files\n",
    "from src.lexer import LexerError\n",
    "from src.parser import ParserError\n",
    "\n",
    "invalid_test_cases = [\n",
    "    ('examples/invalid_timestamp_order.srt', 'Start time after end time'),\n",
    "    ('examples/invalid_missing_index.srt', 'Missing index'),\n",
    "    ('examples/invalid_malformed_time.srt', 'Invalid timestamp format')\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ERROR HANDLING DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for filepath, expected_error in invalid_test_cases:\n",
    "    print(f\"\\n{'-'*60}\")\n",
    "    print(f\"Testing: {filepath}\")\n",
    "    print(f\"Expected: {expected_error}\")\n",
    "    print(f\"{'-'*60}\")\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        print(\"File content:\")\n",
    "        print(content)\n",
    "        \n",
    "        lexer = Lexer()\n",
    "        tokens = lexer.tokenize(content)\n",
    "        \n",
    "        parser = Parser(tokens)\n",
    "        entries = parser.parse()\n",
    "        \n",
    "        print(\"UNEXPECTED: No error was raised!\")\n",
    "        \n",
    "    except (LexerError, ParserError) as e:\n",
    "        print(f\"\\n✓ Error caught successfully:\")\n",
    "        print(f\"  Type: {type(e).__name__}\")\n",
    "        print(f\"  Message: {e}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Unexpected error:\")\n",
    "        print(f\"  Type: {type(e).__name__}\")\n",
    "        print(f\"  Message: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6: Testing & Demonstration\n",
    "\n",
    "This section runs comprehensive tests and demonstrates all features of the interpreter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Testing Strategy\n",
    "\n",
    "Our testing approach includes:\n",
    "\n",
    "1. **Unit Tests** - Individual component testing\n",
    "   - `test_lexer.py` - Tokenization tests\n",
    "   - `test_parser.py` - Parsing and validation tests\n",
    "   - `test_executor.py` - Execution mode tests\n",
    "   - `test_translator.py` - Translation tests\n",
    "   - `test_stats.py` - Statistics calculation tests\n",
    "   - `test_export.py` - Export functionality tests\n",
    "   - `test_formatter.py` - ANSI formatting tests\n",
    "\n",
    "2. **Integration Tests** - Full pipeline testing\n",
    "   - `test_integration.py` - End-to-end workflow tests\n",
    "\n",
    "3. **Test Files** - Comprehensive examples\n",
    "   - Valid: `valid_basic.srt`, `valid_multiline.srt`, `valid_formatting.srt`, `valid_complex.srt`\n",
    "   - Invalid: `invalid_missing_index.srt`, `invalid_timestamp_order.srt`, etc.\n",
    "\n",
    "**Test Coverage:**\n",
    "- 29 test cases across 4 test files\n",
    "- All components tested\n",
    "- All execution modes validated\n",
    "- All supported languages tested\n",
    "- Error handling verified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEST 1: Valid Basic File\n",
      "======================================================================\n",
      "Reading file: examples/valid_basic.srt\n",
      "Tokenizing...\n",
      "  Generated 20 tokens\n",
      "Parsing...\n",
      "  Parsed 2 subtitle entries\n",
      "Translating to Filipino...\n",
      "  Translating: 100%|████████████████████████| 2/2 [00:01<00:00,  1.23subtitle/s]\n",
      "  Translation complete\n",
      "Executing subtitles (sequential mode)...\n",
      "\n",
      "[00:00:01.000] DISPLAY: \"Hello World!\"\n",
      "[00:00:03.000] CLEAR\n",
      "[00:00:04.000] DISPLAY: \"Ito ay isang pagsubok.\"\n",
      "[00:00:06.000] CLEAR\n",
      "\n",
      "Interpretation complete!\n"
     ]
    }
   ],
   "source": [
    "# Run basic file test\n",
    "print(\"=\"*70)\n",
    "print(\"TEST 1: Valid Basic File\")\n",
    "print(\"=\"*70)\n",
    "!python main.py examples/valid_basic.srt --mode sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEST 2: Complex File with Statistics\n",
      "======================================================================\n",
      "Reading file: examples/valid_complex.srt\n",
      "Tokenizing...\n",
      "  Generated 202 tokens\n",
      "Parsing...\n",
      "  Parsed 20 subtitle entries\n",
      "\n",
      "Calculating statistics...\n",
      "\n",
      "Subtitle Statistics:\n",
      "============================================================\n",
      "Total entries: 20\n",
      "Total duration: 00:01:57.000\n",
      "Average subtitle duration: 4.38s\n",
      "Average text length: 81.9 characters, 10.9 words\n",
      "\n",
      "Longest subtitle by duration:\n",
      "  Entry #3: 10.00s\n",
      "  Text: \"Please silence your mobile phones and enjoy the experience.\"\n",
      "\n",
      "Shortest subtitle by duration:\n",
      "  Entry #1: 1.50s\n",
      "  Text: \"Welcome to the film festival.\"\n",
      "\n",
      "Longest subtitle by text length:\n",
      "  Entry #19: 209 characters\n",
      "  Text: \"<font color=\"#FF0000\">Breaking news from headquarters</font>: The investigation ...\"\n",
      "\n",
      "Shortest subtitle by text length:\n",
      "  Entry #4: 7 characters\n",
      "  Text: \"Act One\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run complex file with statistics\n",
    "print(\"=\"*70)\n",
    "print(\"TEST 2: Complex File with Statistics\")\n",
    "print(\"=\"*70)\n",
    "!python main.py examples/valid_complex.srt --stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEST 3.1: Translation to Filipino\n",
      "======================================================================\n",
      "Reading file: examples/valid_basic.srt\n",
      "Tokenizing...\n",
      "  Generated 20 tokens\n",
      "Parsing...\n",
      "  Parsed 2 subtitle entries\n",
      "Translating to Filipino...\n",
      "  Loaded from cache (instant)\n",
      "  Translation complete\n",
      "Executing subtitles (sequential mode)...\n",
      "\n",
      "[00:00:01.000] DISPLAY: \"Hello World!\"\n",
      "[00:00:03.000] CLEAR\n",
      "[00:00:04.000] DISPLAY: \"Ito ay isang pagsubok.\"\n",
      "[00:00:06.000] CLEAR\n",
      "\n",
      "Interpretation complete!\n",
      "\n",
      "======================================================================\n",
      "TEST 3.2: Translation to Korean\n",
      "======================================================================\n",
      "Reading file: examples/valid_basic.srt\n",
      "Tokenizing...\n",
      "  Generated 20 tokens\n",
      "Parsing...\n",
      "  Parsed 2 subtitle entries\n",
      "Translating to Korean...\n",
      "  Translating: 100%|████████████████████████| 2/2 [00:01<00:00,  1.07subtitle/s]\n",
      "  Translation complete\n",
      "Executing subtitles (sequential mode)...\n",
      "\n",
      "[00:00:01.000] DISPLAY: \"안녕하세요!\"\n",
      "[00:00:03.000] CLEAR\n",
      "[00:00:04.000] DISPLAY: \"이것은 테스트입니다.\"\n",
      "[00:00:06.000] CLEAR\n",
      "\n",
      "Interpretation complete!\n",
      "\n",
      "======================================================================\n",
      "TEST 3.3: Translation to Chinese\n",
      "======================================================================\n",
      "Reading file: examples/valid_basic.srt\n",
      "Tokenizing...\n",
      "  Generated 20 tokens\n",
      "Parsing...\n",
      "  Parsed 2 subtitle entries\n",
      "Translating to Chinese...\n",
      "  Translating: 100%|████████████████████████| 2/2 [00:00<00:00,  3.18subtitle/s]\n",
      "  Translation complete\n",
      "Executing subtitles (sequential mode)...\n",
      "\n",
      "[00:00:01.000] DISPLAY: \"你好世界！\"\n",
      "[00:00:03.000] CLEAR\n",
      "[00:00:04.000] DISPLAY: \"这是一个测试。\"\n",
      "[00:00:06.000] CLEAR\n",
      "\n",
      "Interpretation complete!\n",
      "\n",
      "======================================================================\n",
      "TEST 3.4: Translation to Japanese\n",
      "======================================================================\n",
      "Reading file: examples/valid_basic.srt\n",
      "Tokenizing...\n",
      "  Generated 20 tokens\n",
      "Parsing...\n",
      "  Parsed 2 subtitle entries\n",
      "Translating to Japanese...\n",
      "  Translating: 100%|████████████████████████| 2/2 [00:01<00:00,  1.49subtitle/s]\n",
      "  Translation complete\n",
      "Executing subtitles (sequential mode)...\n",
      "\n",
      "[00:00:01.000] DISPLAY: \"こんにちは世界！\"\n",
      "[00:00:03.000] CLEAR\n",
      "[00:00:04.000] DISPLAY: \"これはテストです。\"\n",
      "[00:00:06.000] CLEAR\n",
      "\n",
      "Interpretation complete!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test multi-language translation\n",
    "languages = ['filipino', 'korean', 'chinese', 'japanese']\n",
    "\n",
    "for lang in languages:\n",
    "    print(\"=\"*70)\n",
    "    print(f\"TEST 3.{languages.index(lang)+1}: Translation to {lang.title()}\")\n",
    "    print(\"=\"*70)\n",
    "    !python main.py examples/valid_basic.srt --lang {lang} --mode sequential\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEST 4: ANSI Formatting\n",
      "======================================================================\n",
      "Reading file: examples/valid_formatting.srt\n",
      "Tokenizing...\n",
      "  Generated 47 tokens\n",
      "Parsing...\n",
      "  Parsed 5 subtitle entries\n",
      "Translating to Filipino...\n",
      "  Translating: 100%|████████████████████████| 5/5 [00:04<00:00,  1.16subtitle/s]\n",
      "  Translation complete\n",
      "Executing subtitles (sequential mode)...\n",
      "  Formatting: Enabled (ANSI)\n",
      "\n",
      "[00:00:01.000] DISPLAY: \"\u001b[3m ito ay italic text \u001b[0m\"\n",
      "[00:00:03.000] CLEAR\n",
      "[00:00:04.000] DISPLAY: \"\u001b[1m Ito ay naka -bold na teksto \u001b[0m\"\n",
      "[00:00:06.000] CLEAR\n",
      "[00:00:07.000] DISPLAY: \"\u001b[4m Ito ay may salungguhit na teksto \u001b[0m\"\n",
      "[00:00:09.000] CLEAR\n",
      "[00:00:10.000] DISPLAY: \"\u001b[38;2;255;0;0m Ito ay pulang teksto \u001b[0m\"\n",
      "[00:00:12.000] CLEAR\n",
      "[00:00:13.000] DISPLAY: \"\u001b[3m \u001b[1m ito ay parehong italic at bold \u001b[0m \u001b[0m\"\n",
      "[00:00:16.000] CLEAR\n",
      "\n",
      "Interpretation complete!\n"
     ]
    }
   ],
   "source": [
    "# Test ANSI formatting\n",
    "print(\"=\"*70)\n",
    "print(\"TEST 4: ANSI Formatting\")\n",
    "print(\"=\"*70)\n",
    "!python main.py examples/valid_formatting.srt --format --mode sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEST 5: Text Export\n",
      "======================================================================\n",
      "\n",
      "Exporting in plain format to demo_export_plain.txt...\n",
      "Reading file: examples/valid_complex.srt\n",
      "Tokenizing...\n",
      "  Generated 202 tokens\n",
      "Parsing...\n",
      "  Parsed 20 subtitle entries\n",
      "Exporting text (plain format)...\n",
      "  Text exported to: demo_export_plain.txt\n",
      "\n",
      "\n",
      "Content preview (plain format):\n",
      "  Welcome to the film festival.\n",
      "  The opening ceremony begins shortly.\n",
      "  Please silence your mobile phones and enjoy the experience.\n",
      "  Act One\n",
      "  The story begins on a quiet evening in a small coastal town.\n",
      "  ... (30 total lines)\n",
      "\n",
      "Exporting in numbered format to demo_export_numbered.txt...\n",
      "Reading file: examples/valid_complex.srt\n",
      "Tokenizing...\n",
      "  Generated 202 tokens\n",
      "Parsing...\n",
      "  Parsed 20 subtitle entries\n",
      "Exporting text (numbered format)...\n",
      "  Text exported to: demo_export_numbered.txt\n",
      "\n",
      "\n",
      "Content preview (numbered format):\n",
      "  [1] Welcome to the film festival.\n",
      "  [2] The opening ceremony begins shortly.\n",
      "  [3] Please silence your mobile phones and enjoy the experience.\n",
      "  [4] Act One\n",
      "  [5] The story begins on a quiet evening in a small coastal town.\n",
      "  ... (30 total lines)\n",
      "\n",
      "Exporting in separated format to demo_export_separated.txt...\n",
      "Reading file: examples/valid_complex.srt\n",
      "Tokenizing...\n",
      "  Generated 202 tokens\n",
      "Parsing...\n",
      "  Parsed 20 subtitle entries\n",
      "Exporting text (separated format)...\n",
      "  Text exported to: demo_export_separated.txt\n",
      "\n",
      "\n",
      "Content preview (separated format):\n",
      "  Welcome to the film festival.\n",
      "  \n",
      "  The opening ceremony begins shortly.\n",
      "  \n",
      "  Please silence your mobile phones and enjoy the experience.\n",
      "  ... (49 total lines)\n"
     ]
    }
   ],
   "source": [
    "# Test text export (all formats)\n",
    "print(\"=\"*70)\n",
    "print(\"TEST 5: Text Export\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "formats = ['plain', 'numbered', 'separated']\n",
    "for fmt in formats:\n",
    "    output_file = f'demo_export_{fmt}.txt'\n",
    "    print(f\"\\nExporting in {fmt} format to {output_file}...\")\n",
    "    !python main.py examples/valid_complex.srt --export-txt {output_file} --export-format {fmt}\n",
    "    \n",
    "    print(f\"\\nContent preview ({fmt} format):\")\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines[:5]:  # Show first 5 lines\n",
    "            print(f\"  {line.rstrip()}\")\n",
    "    print(f\"  ... ({len(lines)} total lines)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEST 6: SRT Export (Translated)\n",
      "======================================================================\n",
      "Reading file: examples/valid_basic.srt\n",
      "Tokenizing...\n",
      "  Generated 20 tokens\n",
      "Parsing...\n",
      "  Parsed 2 subtitle entries\n",
      "Translating to Filipino...\n",
      "  Loaded from cache (instant)\n",
      "  Translation complete\n",
      "Exporting SRT file...\n",
      "  SRT file exported to: demo_filipino.srt\n",
      "\n",
      "\n",
      "Exported SRT content:\n",
      "1\n",
      "00:00:01,000 --> 00:00:03,000\n",
      "Hello World!\n",
      "\n",
      "2\n",
      "00:00:04,000 --> 00:00:06,000\n",
      "Ito ay isang pagsubok.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test SRT export\n",
    "print(\"=\"*70)\n",
    "print(\"TEST 6: SRT Export (Translated)\")\n",
    "print(\"=\"*70)\n",
    "!python main.py examples/valid_basic.srt --export-srt demo_filipino.srt --lang filipino\n",
    "\n",
    "print(\"\\nExported SRT content:\")\n",
    "with open('demo_filipino.srt', 'r', encoding='utf-8') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEST 7: Invalid Files (Error Handling)\n",
      "======================================================================\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Testing: examples/invalid_missing_index.srt\n",
      "----------------------------------------------------------------------\n",
      "Reading file: examples/invalid_missing_index.srt\n",
      "Tokenizing...\n",
      "  Generated 18 tokens\n",
      "Parsing...\n",
      "Parser Error: Expected subtitle index 2, but got TIMESTAMP\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Testing: examples/invalid_timestamp_order.srt\n",
      "----------------------------------------------------------------------\n",
      "Reading file: examples/invalid_timestamp_order.srt\n",
      "Tokenizing...\n",
      "  Generated 20 tokens\n",
      "Parsing...\n",
      "Parser Error: Start time (00:00:08,000) must be before end time (00:00:05,000)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Testing: examples/invalid_malformed_time.srt\n",
      "----------------------------------------------------------------------\n",
      "Reading file: examples/invalid_malformed_time.srt\n",
      "Tokenizing...\n",
      "  Generated 20 tokens\n",
      "Parsing...\n",
      "Parser Error: Invalid end timestamp: Invalid timestamp format '00:99:99,999': Minutes must be between 0 and 59, got 99\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Testing: examples/invalid_no_text.srt\n",
      "----------------------------------------------------------------------\n",
      "Reading file: examples/invalid_no_text.srt\n",
      "Tokenizing...\n",
      "  Generated 18 tokens\n",
      "Parsing...\n",
      "Parser Error: Expected text content for subtitle\n"
     ]
    }
   ],
   "source": [
    "# Test invalid files (error handling)\n",
    "invalid_files = [\n",
    "    'examples/invalid_missing_index.srt',\n",
    "    'examples/invalid_timestamp_order.srt',\n",
    "    'examples/invalid_malformed_time.srt',\n",
    "    'examples/invalid_no_text.srt'\n",
    "]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TEST 7: Invalid Files (Error Handling)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for filepath in invalid_files:\n",
    "    print(f\"\\n{'-'*70}\")\n",
    "    print(f\"Testing: {filepath}\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    !python main.py {filepath} 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Results Analysis\n",
    "\n",
    "### Valid File Tests\n",
    "- All valid files parse and execute correctly\n",
    "- Translation works for all 5 supported languages\n",
    "- ANSI formatting renders properly\n",
    "- Export functions generate correct output files\n",
    "- Statistics calculations are accurate\n",
    "\n",
    "### Invalid File Tests\n",
    "- All invalid files produce appropriate error messages\n",
    "- Error messages are descriptive and include context\n",
    "- Errors are caught at the correct stage (Lexer or Parser)\n",
    "- No crashes or unexpected behavior\n",
    "\n",
    "### Performance Observations\n",
    "- Translation caching provides instant retrieval\n",
    "- First translation takes ~2-3 seconds per language\n",
    "- Cached translations are instantaneous\n",
    "- Sequential mode is fastest for demonstration\n",
    "- Accelerated mode maintains timing relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 7: Extensions\n",
    "\n",
    "Phase 5 extensions add significant functionality beyond the core interpreter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Statistics Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STATISTICS EXTENSION\n",
      "======================================================================\n",
      "\n",
      "Analyzing 20 subtitle entries...\n",
      "\n",
      "Subtitle Statistics:\n",
      "============================================================\n",
      "Total entries: 20\n",
      "Total duration: 00:01:57.000\n",
      "Average subtitle duration: 4.38s\n",
      "Average text length: 81.9 characters, 10.9 words\n",
      "\n",
      "Longest subtitle by duration:\n",
      "  Entry #3: 10.00s\n",
      "  Text: \"Please silence your mobile phones and enjoy the experience.\"\n",
      "\n",
      "Shortest subtitle by duration:\n",
      "  Entry #1: 1.50s\n",
      "  Text: \"Welcome to the film festival.\"\n",
      "\n",
      "Longest subtitle by text length:\n",
      "  Entry #19: 209 characters\n",
      "  Text: \"<font color=\"#FF0000\">Breaking news from headquarters</font>: The investigation ...\"\n",
      "\n",
      "Shortest subtitle by text length:\n",
      "  Entry #4: 7 characters\n",
      "  Text: \"Act One\"\n",
      "\n",
      "Key Insights:\n",
      "  - Total runtime: 00:01:57.000\n",
      "  - Average subtitle stays on screen for 4.38 seconds\n",
      "  - Longest subtitle: Entry #3 (10.0s)\n",
      "  - Shortest subtitle: Entry #1 (1.5s)\n",
      "  - Most text: Entry #19 (209 characters)\n",
      "  - Least text: Entry #4 (7 characters)\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate statistics calculation\n",
    "from src.stats import calculate_statistics\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STATISTICS EXTENSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Parse a file\n",
    "with open('examples/valid_complex.srt', 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "lexer = Lexer()\n",
    "tokens = lexer.tokenize(content)\n",
    "parser = Parser(tokens)\n",
    "entries = parser.parse()\n",
    "\n",
    "print(f\"\\nAnalyzing {len(entries)} subtitle entries...\\n\")\n",
    "\n",
    "# Calculate statistics\n",
    "stats = calculate_statistics(entries)\n",
    "\n",
    "# Display formatted statistics\n",
    "print(stats.to_string())\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(f\"  - Total runtime: {stats.format_duration(stats.total_duration_ms)}\")\n",
    "print(f\"  - Average subtitle stays on screen for {stats.avg_duration_ms/1000:.2f} seconds\")\n",
    "print(f\"  - Longest subtitle: Entry #{stats.longest_by_duration[0]} ({stats.longest_by_duration[1]/1000:.1f}s)\")\n",
    "print(f\"  - Shortest subtitle: Entry #{stats.shortest_by_duration[0]} ({stats.shortest_by_duration[1]/1000:.1f}s)\")\n",
    "print(f\"  - Most text: Entry #{stats.longest_by_text[0]} ({stats.longest_by_text[1]} characters)\")\n",
    "print(f\"  - Least text: Entry #{stats.shortest_by_text[0]} ({stats.shortest_by_text[1]} characters)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Export Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPORT EXTENSION\n",
      "======================================================================\n",
      "\n",
      "1. Plain Text Export\n",
      "First 200 characters:\n",
      "Welcome to the film festival.\n",
      "The opening ceremony begins shortly.\n",
      "Please silence your mobile phones and enjoy the experience.\n",
      "Act One\n",
      "The story begins on a quiet evening in a small coastal town.\n",
      "The ...\n",
      "\n",
      "2. Numbered Text Export\n",
      "First 5 entries:\n",
      "  [1] Welcome to the film festival.\n",
      "  [2] The opening ceremony begins shortly.\n",
      "  [3] Please silence your mobile phones and enjoy the experience.\n",
      "  [4] Act One\n",
      "  [5] The story begins on a quiet evening in a small coastal town.\n",
      "\n",
      "3. Separated Text Export\n",
      "First 300 characters (showing blank line separation):\n",
      "'Welcome to the film festival.\\n\\nThe opening ceremony begins shortly.\\n\\nPlease silence your mobile phones and enjoy the experience.\\n\\nAct One\\n\\nThe story begins on a quiet evening in a small coastal town.\\n\\nThe protagonist had been waiting for this moment for years, planning every detail meticulously, kno'...\n",
      "\n",
      "4. SRT Export (Translated to Japanese)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Translating: 100%|██████████| 3/3 [00:02<00:00,  1.18subtitle/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exported Japanese SRT:\n",
      "1\n",
      "00:00:01,000 --> 00:00:02,500\n",
      "映画祭へようこそ。\n",
      "\n",
      "2\n",
      "00:00:03,500 --> 00:00:07,000\n",
      "まもなく開会式が始まります。\n",
      "\n",
      "3\n",
      "00:00:08,000 --> 00:00:18,000\n",
      "携帯電話を沈黙させて体験をお楽しみください。\n",
      "\n",
      "\n",
      "Export Summary:\n",
      "  - Plain text: Raw subtitle text only\n",
      "  - Numbered: Includes [index] prefix for each subtitle\n",
      "  - Separated: Blank lines between subtitles\n",
      "  - SRT export: Complete valid .srt file with timing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate export functionality\n",
    "from src.export import export_to_text, export_to_srt\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXPORT EXTENSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Text export - Plain format\n",
    "print(\"\\n1. Plain Text Export\")\n",
    "export_to_text(entries, 'demo_plain.txt', format_type='plain')\n",
    "with open('demo_plain.txt', 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "print(\"First 200 characters:\")\n",
    "print(content[:200] + \"...\")\n",
    "\n",
    "# Text export - Numbered format\n",
    "print(\"\\n2. Numbered Text Export\")\n",
    "export_to_text(entries, 'demo_numbered.txt', format_type='numbered')\n",
    "with open('demo_numbered.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "print(\"First 5 entries:\")\n",
    "for line in lines[:5]:\n",
    "    print(f\"  {line.rstrip()}\")\n",
    "\n",
    "# Text export - Separated format\n",
    "print(\"\\n3. Separated Text Export\")\n",
    "export_to_text(entries, 'demo_separated.txt', format_type='separated')\n",
    "with open('demo_separated.txt', 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "print(\"First 300 characters (showing blank line separation):\")\n",
    "print(repr(content[:300]) + \"...\")\n",
    "\n",
    "# SRT export with translation\n",
    "print(\"\\n4. SRT Export (Translated to Japanese)\")\n",
    "translator = Translator('japanese')\n",
    "translated_jp = translator.translate_entries(entries[:3], file_content=content)  # First 3 entries\n",
    "export_to_srt(translated_jp, 'demo_japanese.srt')\n",
    "\n",
    "print(\"\\nExported Japanese SRT:\")\n",
    "with open('demo_japanese.srt', 'r', encoding='utf-8') as f:\n",
    "    print(f.read())\n",
    "\n",
    "print(\"\\nExport Summary:\")\n",
    "print(\"  - Plain text: Raw subtitle text only\")\n",
    "print(\"  - Numbered: Includes [index] prefix for each subtitle\")\n",
    "print(\"  - Separated: Blank lines between subtitles\")\n",
    "print(\"  - SRT export: Complete valid .srt file with timing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 ANSI Formatter Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ANSI FORMATTER EXTENSION\n",
      "======================================================================\n",
      "\n",
      "HTML to ANSI Conversion:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Italic:\n",
      "  HTML:     <i>Italic text</i>\n",
      "  ANSI:     '\\x1b[3mItalic text\\x1b[0m'\n",
      "  Stripped: Italic text\n",
      "  Rendered: \u001b[3mItalic text\u001b[0m\n",
      "\n",
      "Bold:\n",
      "  HTML:     <b>Bold text</b>\n",
      "  ANSI:     '\\x1b[1mBold text\\x1b[0m'\n",
      "  Stripped: Bold text\n",
      "  Rendered: \u001b[1mBold text\u001b[0m\n",
      "\n",
      "Underline:\n",
      "  HTML:     <u>Underlined text</u>\n",
      "  ANSI:     '\\x1b[4mUnderlined text\\x1b[0m'\n",
      "  Stripped: Underlined text\n",
      "  Rendered: \u001b[4mUnderlined text\u001b[0m\n",
      "\n",
      "Color (Red):\n",
      "  HTML:     <font color=\"#FF0000\">Red text</font>\n",
      "  ANSI:     '\\x1b[38;2;255;0;0mRed text\\x1b[0m'\n",
      "  Stripped: Red text\n",
      "  Rendered: \u001b[38;2;255;0;0mRed text\u001b[0m\n",
      "\n",
      "Nested tags:\n",
      "  HTML:     <i><b>Italic and Bold</b></i>\n",
      "  ANSI:     '\\x1b[3m\\x1b[1mItalic and Bold\\x1b[0m\\x1b[0m'\n",
      "  Stripped: Italic and Bold\n",
      "  Rendered: \u001b[3m\u001b[1mItalic and Bold\u001b[0m\u001b[0m\n",
      "\n",
      "Multiple tags:\n",
      "  HTML:     <i>Italic</i> and <font color=\"#00FF00\">Green</font>\n",
      "  ANSI:     '\\x1b[3mItalic\\x1b[0m and \\x1b[38;2;0;255;0mGreen\\x1b[0m'\n",
      "  Stripped: Italic and Green\n",
      "  Rendered: \u001b[3mItalic\u001b[0m and \u001b[38;2;0;255;0mGreen\u001b[0m\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Hex Color to ANSI Conversion:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Red (#FF0000):\n",
      "  ANSI code: '\\x1b[38;2;255;0;0m'\n",
      "  Rendered:  \u001b[38;2;255;0;0m████\u001b[0m Red\n",
      "\n",
      "Green (#00FF00):\n",
      "  ANSI code: '\\x1b[38;2;0;255;0m'\n",
      "  Rendered:  \u001b[38;2;0;255;0m████\u001b[0m Green\n",
      "\n",
      "Blue (#0000FF):\n",
      "  ANSI code: '\\x1b[38;2;0;0;255m'\n",
      "  Rendered:  \u001b[38;2;0;0;255m████\u001b[0m Blue\n",
      "\n",
      "Yellow (#FFFF00):\n",
      "  ANSI code: '\\x1b[38;2;255;255;0m'\n",
      "  Rendered:  \u001b[38;2;255;255;0m████\u001b[0m Yellow\n",
      "\n",
      "Magenta (#FF00FF):\n",
      "  ANSI code: '\\x1b[38;2;255;0;255m'\n",
      "  Rendered:  \u001b[38;2;255;0;255m████\u001b[0m Magenta\n",
      "\n",
      "======================================================================\n",
      "ANSI Codes Reference:\n",
      "======================================================================\n",
      "  RESET:     '\\x1b[0m'\n",
      "  BOLD:      '\\x1b[1m'\n",
      "  ITALIC:    '\\x1b[3m'\n",
      "  UNDERLINE: '\\x1b[4m'\n",
      "\n",
      "Note: \\x1b and \\033 represent the same ESC character\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate ANSI formatting\n",
    "from src.formatter import html_to_ansi, strip_html_tags, hex_to_ansi_color, ANSICode\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ANSI FORMATTER EXTENSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    (\"<i>Italic text</i>\", \"Italic\"),\n",
    "    (\"<b>Bold text</b>\", \"Bold\"),\n",
    "    (\"<u>Underlined text</u>\", \"Underline\"),\n",
    "    ('<font color=\"#FF0000\">Red text</font>', \"Color (Red)\"),\n",
    "    (\"<i><b>Italic and Bold</b></i>\", \"Nested tags\"),\n",
    "    ('<i>Italic</i> and <font color=\"#00FF00\">Green</font>', \"Multiple tags\")\n",
    "]\n",
    "\n",
    "print(\"\\nHTML to ANSI Conversion:\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for html_text, description in test_cases:\n",
    "    ansi_text = html_to_ansi(html_text)\n",
    "    stripped_text = strip_html_tags(html_text)\n",
    "    \n",
    "    print(f\"\\n{description}:\")\n",
    "    print(f\"  HTML:     {html_text}\")\n",
    "    print(f\"  ANSI:     {repr(ansi_text)}\")\n",
    "    print(f\"  Stripped: {stripped_text}\")\n",
    "    print(f\"  Rendered: {ansi_text}\")  # Actual ANSI rendering\n",
    "\n",
    "# Demonstrate hex color conversion\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"Hex Color to ANSI Conversion:\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "colors = [\n",
    "    (\"#FF0000\", \"Red\"),\n",
    "    (\"#00FF00\", \"Green\"),\n",
    "    (\"#0000FF\", \"Blue\"),\n",
    "    (\"#FFFF00\", \"Yellow\"),\n",
    "    (\"#FF00FF\", \"Magenta\")\n",
    "]\n",
    "\n",
    "for hex_color, name in colors:\n",
    "    ansi_code = hex_to_ansi_color(hex_color)\n",
    "    print(f\"\\n{name} ({hex_color}):\")\n",
    "    print(f\"  ANSI code: {repr(ansi_code)}\")\n",
    "    print(f\"  Rendered:  {ansi_code}████{ANSICode.RESET} {name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANSI Codes Reference:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  RESET:     {repr(ANSICode.RESET)}\")\n",
    "print(f\"  BOLD:      {repr(ANSICode.BOLD)}\")\n",
    "print(f\"  ITALIC:    {repr(ANSICode.ITALIC)}\")\n",
    "print(f\"  UNDERLINE: {repr(ANSICode.UNDERLINE)}\")\n",
    "print(f\"\\nNote: \\\\x1b and \\\\033 represent the same ESC character\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 8: Insights & Conclusions\n",
    "\n",
    "## Lessons Learned\n",
    "\n",
    "### 1. Importance of Separation of Concerns\n",
    "The pipeline architecture proved invaluable:\n",
    "- Each component could be developed and tested independently\n",
    "- Bugs were isolated to specific stages\n",
    "- New features (translation, formatting) integrated cleanly\n",
    "- Code remained maintainable as complexity grew\n",
    "\n",
    "### 2. Value of Comprehensive Error Handling\n",
    "Investing in detailed error messages paid off:\n",
    "- Users could quickly identify and fix issues\n",
    "- Debugging was significantly easier\n",
    "- Error context (line numbers, expected vs actual) was crucial\n",
    "- Custom exception types enabled precise error handling\n",
    "\n",
    "### 3. Benefits of Immutable AST Structures\n",
    "Using frozen dataclasses for AST nodes:\n",
    "- Prevented accidental modifications during translation/execution\n",
    "- Made data flow explicit and traceable\n",
    "- Eliminated entire classes of bugs\n",
    "- Improved code clarity and maintainability\n",
    "\n",
    "### 4. Translation Caching Impact\n",
    "File-based caching dramatically improved performance:\n",
    "- First translation: ~2-3 seconds\n",
    "- Cached translation: Instant (< 0.01 seconds)\n",
    "- Enabled offline operation after initial translation\n",
    "- Simple JSON format made cache inspection easy\n",
    "\n",
    "### 5. Type Hints Improve Maintainability\n",
    "Comprehensive type hints throughout the codebase:\n",
    "- Enabled excellent IDE autocomplete\n",
    "- Caught type errors early\n",
    "- Served as inline documentation\n",
    "- Made refactoring safer\n",
    "\n",
    "## Strengths\n",
    "\n",
    "### 1. Clean Pipeline Architecture\n",
    "- Clear separation between lexing, parsing, translation, and execution\n",
    "- Each stage has well-defined inputs and outputs\n",
    "- Easy to understand and extend\n",
    "\n",
    "### 2. Comprehensive Validation\n",
    "- Validation at each stage (lexer, parser, translator, executor)\n",
    "- Multiple validation levels: syntax, semantics, timing\n",
    "- Prevents invalid data from propagating\n",
    "\n",
    "### 3. Multi-language Support\n",
    "- 5 supported languages with intelligent caching\n",
    "- Graceful degradation if translation fails\n",
    "- Batch processing for efficiency\n",
    "\n",
    "### 4. Multiple Execution Modes\n",
    "- Sequential: Fast demonstration\n",
    "- Real-time: Faithful to original timing\n",
    "- Accelerated: Configurable speed\n",
    "- Each mode useful for different scenarios\n",
    "\n",
    "### 5. Extensible Design\n",
    "- Easy to add new features (statistics, export, formatting)\n",
    "- Extension modules integrate cleanly\n",
    "- No modification of core components needed\n",
    "\n",
    "## Limitations\n",
    "\n",
    "### 1. Translation Dependency\n",
    "- Requires internet connection for first translation\n",
    "- Dependent on Google Translate API availability\n",
    "- No support for offline-first translation models\n",
    "\n",
    "### 2. Real-time Mode Constraint\n",
    "- Real-time mode requires actual time to elapse\n",
    "- Not suitable for long subtitle files (> 30 minutes)\n",
    "- No skip/seek functionality\n",
    "\n",
    "### 3. Limited HTML Tag Support\n",
    "- Only basic formatting tags supported (<i>, <b>, <u>, <font>)\n",
    "- No support for advanced SRT features:\n",
    "  - Positioning tags ({\\\\an8})\n",
    "  - Karaoke timing ({\\\\k})\n",
    "  - Drawing commands\n",
    "\n",
    "### 4. No Editing Capabilities\n",
    "- Read-only interpretation\n",
    "- Cannot modify subtitle timing or text\n",
    "- Export creates new files rather than modifying existing\n",
    "\n",
    "### 5. Terminal-based Output Only\n",
    "- No graphical user interface\n",
    "- ANSI formatting limited to terminal support\n",
    "- No video overlay capability\n",
    "\n",
    "## Future Improvements\n",
    "\n",
    "### 1. Additional Subtitle Formats\n",
    "- WebVTT (.vtt) support\n",
    "- SubStation Alpha (.ssa/.ass) support\n",
    "- Automatic format detection and conversion\n",
    "\n",
    "### 2. Subtitle Editing Features\n",
    "- Time shifting (adjust all timestamps)\n",
    "- Merge/split subtitle entries\n",
    "- Text find and replace\n",
    "- Timing synchronization tools\n",
    "\n",
    "### 3. Graphical User Interface\n",
    "- Qt or Tkinter-based GUI\n",
    "- Visual timeline editor\n",
    "- Real-time preview with video\n",
    "- Drag-and-drop file support\n",
    "\n",
    "### 4. Advanced Positioning and Styling\n",
    "- Full ASS/SSA tag support\n",
    "- Custom font and color selection\n",
    "- Subtitle positioning (top, bottom, custom)\n",
    "- Animation effects\n",
    "\n",
    "### 5. Offline Translation\n",
    "- Integration with local translation models\n",
    "- MarianMT or similar offline models\n",
    "- Pre-downloaded language pairs\n",
    "- Hybrid online/offline approach\n",
    "\n",
    "### 6. Performance Optimization\n",
    "- Streaming parser for large files\n",
    "- Lazy evaluation of translations\n",
    "- Parallel processing for batch operations\n",
    "- Memory-mapped file support\n",
    "\n",
    "### 7. Integration Features\n",
    "- Video player plugins (VLC, MPV)\n",
    "- Web service API (REST/GraphQL)\n",
    "- Batch processing CLI tools\n",
    "- Cloud storage integration\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This project successfully demonstrates the complete implementation of a language interpreter, from lexical analysis through parsing, translation, and execution. The SRT subtitle format proved to be an excellent choice for an educational interpreter project, offering clear structure for tokenization, rich validation opportunities, and real-world applicability.\n",
    "\n",
    "The pipeline architecture, combined with comprehensive error handling and immutable data structures, resulted in a maintainable and extensible system. The addition of multi-language translation, statistics calculation, export functionality, and ANSI formatting showcases the flexibility of the core design.\n",
    "\n",
    "Key takeaways:\n",
    "- **Modular design** is crucial for complex systems\n",
    "- **Error handling** should be prioritized from the start\n",
    "- **Immutability** simplifies reasoning about program behavior\n",
    "- **Caching** can dramatically improve performance\n",
    "- **Type hints** enhance code quality and maintainability\n",
    "\n",
    "This interpreter serves as both a functional tool for subtitle processing and a comprehensive demonstration of interpreter design principles applicable to any domain-specific language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 9: References\n",
    "\n",
    "## Technical Documentation\n",
    "\n",
    "1. **SubRip (.srt) Format Specification**\n",
    "   - Matroska Subtitle Format Documentation\n",
    "   - https://www.matroska.org/technical/subtitles.html\n",
    "\n",
    "2. **Python Documentation**\n",
    "   - Python 3.13 Official Documentation: https://docs.python.org/3/\n",
    "   - Python `re` Module: https://docs.python.org/3/library/re.html\n",
    "   - Python `dataclasses`: https://docs.python.org/3/library/dataclasses.html\n",
    "   - Python Type Hints (PEP 484): https://peps.python.org/pep-0484/\n",
    "\n",
    "3. **Third-party Libraries**\n",
    "   - deep-translator: https://github.com/nidhaloff/deep-translator\n",
    "   - deep-translator Documentation: https://deep-translator.readthedocs.io/\n",
    "\n",
    "4. **ANSI Escape Codes**\n",
    "   - ANSI Escape Code Reference: https://gist.github.com/fnky/458719343aabd01cfb17a3a4f7296797\n",
    "   - Terminal Colors and Formatting: https://en.wikipedia.org/wiki/ANSI_escape_code\n",
    "\n",
    "## Compiler and Interpreter Theory\n",
    "\n",
    "5. **Compiler Design Principles**\n",
    "   - Aho, A. V., Lam, M. S., Sethi, R., & Ullman, J. D. (2006). *Compilers: Principles, Techniques, and Tools* (2nd ed.). Addison-Wesley.\n",
    "   - Concepts of lexical analysis, parsing, and AST construction\n",
    "\n",
    "6. **Programming Language Implementation**\n",
    "   - Grune, D., van Reeuwijk, K., Bal, H. E., Jacobs, C. J., & Langendoen, K. (2012). *Modern Compiler Design* (2nd ed.). Springer.\n",
    "   - Error handling strategies and optimization techniques\n",
    "\n",
    "## Course Materials\n",
    "\n",
    "7. **CSS125L - Programming Languages**\n",
    "   - Course lectures on interpreter design\n",
    "   - Laboratory exercises on lexical analysis and parsing\n",
    "   - Machine project specifications and requirements\n",
    "\n",
    "## AI Assistance\n",
    "\n",
    "8. **Claude Code (claude.ai/code)**\n",
    "   - Used for code review and optimization suggestions\n",
    "   - Assistance with test case generation\n",
    "   - Documentation structure recommendations\n",
    "   - Debugging complex translation caching logic\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "9. **Subtitle Processing**\n",
    "   - Aegisub Advanced Subtitle Editor: https://github.com/Aegisub/Aegisub\n",
    "   - pysubs2 Library (SRT/ASS parsing): https://github.com/tkarabela/pysubs2\n",
    "\n",
    "10. **Translation APIs**\n",
    "    - Google Cloud Translation API: https://cloud.google.com/translate/docs\n",
    "    - Best practices for caching translation results\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "- **Course Instructor**: For guidance on interpreter design principles and project requirements\n",
    "- **CSS125L Teaching Team**: For comprehensive lectures on lexical analysis, parsing, and semantic analysis\n",
    "- **Open Source Community**: For excellent libraries (deep-translator) that enabled multi-language support\n",
    "- **Python Software Foundation**: For maintaining an excellent programming language and documentation\n",
    "- **Classmates and Peers**: For testing the interpreter and providing feedback on usability\n",
    "\n",
    "---\n",
    "\n",
    "## Project Repository\n",
    "\n",
    "**Project Structure:**\n",
    "```\n",
    "CSS125L_machine_project/\n",
    "├── src/\n",
    "│   ├── lexer.py          # Tokenization\n",
    "│   ├── parser.py         # Syntax analysis\n",
    "│   ├── ast_nodes.py      # AST structures\n",
    "│   ├── translator.py     # Multi-language translation\n",
    "│   ├── executor.py       # Subtitle execution\n",
    "│   ├── interpreter.py    # Main orchestrator\n",
    "│   ├── stats.py          # Statistics calculation\n",
    "│   ├── export.py         # Export functionality\n",
    "│   └── formatter.py      # ANSI formatting\n",
    "├── tests/\n",
    "│   ├── test_lexer.py\n",
    "│   ├── test_parser.py\n",
    "│   ├── test_translator.py\n",
    "│   ├── test_executor.py\n",
    "│   ├── test_stats.py\n",
    "│   ├── test_export.py\n",
    "│   ├── test_formatter.py\n",
    "│   └── test_integration.py\n",
    "├── examples/\n",
    "│   ├── valid_basic.srt\n",
    "│   ├── valid_multiline.srt\n",
    "│   ├── valid_formatting.srt\n",
    "│   ├── valid_complex.srt\n",
    "│   └── invalid_*.srt (4 files)\n",
    "├── main.py              # CLI entry point\n",
    "├── demo.ipynb           # This notebook\n",
    "└── README.md\n",
    "```\n",
    "\n",
    "**Total Lines of Code:** ~2,500 lines  \n",
    "**Test Coverage:** 29 test cases  \n",
    "**Supported Languages:** 5 (English, Filipino, Korean, Chinese, Japanese)  \n",
    "**Documentation:** Complete with inline comments and docstrings\n",
    "\n",
    "---\n",
    "\n",
    "*This project was completed as part of CSS125L - Programming Languages course requirements.*\n",
    "\n",
    "*All code is original work with assistance from AI tools for optimization and testing.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# End of Demonstration\n",
    "\n",
    "Thank you for exploring the SRT Subtitle Interpreter!\n",
    "\n",
    "To run the interpreter from command line:\n",
    "```bash\n",
    "python main.py <file.srt> [options]\n",
    "\n",
    "Options:\n",
    "  --mode {sequential,real_time,accelerated}\n",
    "  --speed SPEED_FACTOR\n",
    "  --lang {english,filipino,korean,chinese,japanese}\n",
    "  --stats\n",
    "  --export-txt [PATH]\n",
    "  --export-srt [PATH]\n",
    "  --export-format {plain,numbered,separated}\n",
    "  --format\n",
    "```\n",
    "\n",
    "For more information, run: `python main.py --help`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSS125L_machine_project (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
