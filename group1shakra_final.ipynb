{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SRT Subtitle Interpreter System\n",
    "\n",
    "**Group 1 - Shakra**\n",
    "- Bagallon, Radzie, R.\n",
    "- Castro, Joselito Miguel C.\n",
    "- Duldulao, Jacob O.\n",
    "- Gigante, Raphael Nicolai M.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Introduction to the Problem/Task and Interpreter System\n",
    "\n",
    "### What is an Interpreter?\n",
    "\n",
    "An interpreter is a program that reads and executes code written in a specific language. It is different from compilers that translate entire programs all at once, interpreters process the input line by line.\n",
    "\n",
    "Some examples of where interpreters are used are the following:\n",
    "\n",
    "- Python itself uses an interpreter to run code\n",
    "- Command shells (like bash) interpret terminal commands\n",
    "- Web browsers interpret HTML and JavaScript\n",
    "- Game engines interpret scripting languages\n",
    "\n",
    "Interpreters are important because they make it easier to work with structured data and execute instructions without needing to understand low-level machine code.\n",
    "\n",
    "### SRT Subtitle System\n",
    "\n",
    "Because we like watching subbed media (mostly anime and k-drama), we chose to build an SRT subtitle interpreter. This is a system that reads, validates, and displays subtitle files. For this project, we're specifically working with the SRT (SubRip) format. It is one of the most popular subtitle formats because it's simple, text-based, and widely supported.\n",
    "\n",
    "Real-world applications:\n",
    "\n",
    "- Video players (VLC, Windows Media Player) parse SRT files to display timed text\n",
    "- Streaming platforms (Netflix, YouTube) use subtitle interpreters for accessibility\n",
    "- Video editors rely on subtitle parsers to sync captions with video\n",
    "- Translation tools process subtitle files to convert dialogue between languages\n",
    "\n",
    "Target tasks:\n",
    "\n",
    "1. Read SRT files and break them into meaningful pieces (tokens)\n",
    "2. Validate the structure to ensure files follow the correct format\n",
    "3. Execute the subtitles by displaying them with proper timing\n",
    "4. Translate subtitles to different languages for multilingual support\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Description of the Input Language\n",
    "\n",
    "### What is the SRT Format?\n",
    "\n",
    "The SRT (SubRip) format is a simple text-based subtitle format created for extracting subtitles from video files. It's designed to be human-readable and easy to edit with any text editor.\n",
    "\n",
    "### Structure of an SRT File\n",
    "\n",
    "Every SRT file follows a pattern. Each subtitle entry has four parts:\n",
    "\n",
    "```\n",
    "1                                    ← Index number\n",
    "00:00:01,000 --> 00:00:03,000        ← Timestamp (start --> end)\n",
    "Hello, world!                        ← Text content (can be multiple lines)\n",
    "                                     ← Blank line separator\n",
    "2\n",
    "00:00:04,000 --> 00:00:06,000\n",
    "Welcome to our demo.\n",
    "\n",
    "```\n",
    "\n",
    "### Tokens Recognized by Our Interpreter\n",
    "\n",
    "Our lexer (tokenizer) recognizes these token types:\n",
    "\n",
    "| Token Type | Description                       | Example         |\n",
    "| ---------- | --------------------------------- | --------------- |\n",
    "| INDEX      | Sequential subtitle number        | `1`, `2`, `3`   |\n",
    "| TIMESTAMP  | Time in format HH:MM:SS,MMM       | `00:00:01,500`  |\n",
    "| ARROW      | Separator between start/end times | `-->`           |\n",
    "| TEXT       | Subtitle content                  | `Hello, world!` |\n",
    "| NEWLINE    | Line break                        | `\\n`            |\n",
    "| BLANK_LINE | Empty line (separator)            | ` `             |\n",
    "| EOF        | End of file marker                | (none)          |\n",
    "\n",
    "### Grammar and Syntax Rules\n",
    "\n",
    "Valid subtitle structure:\n",
    "\n",
    "```\n",
    "subtitle_entry → INDEX NEWLINE timestamp_line NEWLINE text_lines BLANK_LINE\n",
    "timestamp_line → TIMESTAMP ARROW TIMESTAMP\n",
    "text_lines → TEXT NEWLINE (TEXT NEWLINE)*\n",
    "```\n",
    "\n",
    "Rules for valid statements:\n",
    "\n",
    "1. Index numbers must be sequential (1, 2, 3, ...)\n",
    "2. Timestamps must follow format: `HH:MM:SS,MMM` (e.g., `00:01:30,500`)\n",
    "3. Start time must come before end time\n",
    "4. Each subtitle must have at least one line of text\n",
    "5. Subtitles must be separated by a blank line\n",
    "6. Minutes and seconds must be ≤ 59; milliseconds must be ≤ 999\n",
    "\n",
    "### Examples: Valid vs Invalid\n",
    "\n",
    "#### Valid Input\n",
    "\n",
    "```\n",
    "1\n",
    "00:00:01,000 --> 00:00:03,000\n",
    "This is a valid subtitle.\n",
    "\n",
    "```\n",
    "\n",
    "#### Invalid Inputs\n",
    "\n",
    "```\n",
    "00:00:01,000 --> 00:00:03,000\n",
    "Missing the index number!\n",
    "\n",
    "1\n",
    "00:00:05,000 --> 00:00:02,000\n",
    "End time comes before start time!\n",
    "\n",
    "1\n",
    "00:99:01,000 --> 00:00:03,000\n",
    "Minutes can't be 99!\n",
    "\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: System Design\n",
    "\n",
    "### Built-in Python Libraries\n",
    "\n",
    "| Library | Purpose                | Description                                                    |\n",
    "| ------- | ---------------------- | -------------------------------------------------------------- |\n",
    "| `re`    | Regular expressions    | Pattern matching for timestamps and index numbers in the lexer |\n",
    "| `time`  | Time-related functions | Adding delays between subtitle displays to simulate timing     |\n",
    "\n",
    "### Third-Party Libraries\n",
    "\n",
    "| Library           | Purpose                              | Description                                                            |\n",
    "| ----------------- | ------------------------------------ | ---------------------------------------------------------------------- |\n",
    "| `deep-translator` | Translation via Google Translate API | Translating subtitle text to Filipino, Korean, Chinese, Japanese, etc. |\n",
    "\n",
    "We used `deep-translator` because it's free and doesn't require API keys. The tradeoff is that the translations tend to be literal rather than contextually aware, missing some nuance in the process.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Data Preprocessing and Cleaning\n",
    "\n",
    "Our interpreter follows the classic three-stage pipeline used in most compiler and interpreter designs:\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Input - SRT File]\n",
    "    A --> B[LEXER / Tokenizer<br/><i>Breaks text into tokens</i>]\n",
    "    B --> C[Tokens]\n",
    "    C --> D[PARSER<br/><i>Validates structure, builds AST</i>]\n",
    "    D --> E[Subtitle Entries]\n",
    "    E --> F[EXECUTOR<br/><i>Displays subtitles with optional translation</i>]\n",
    "    F --> G[Output]\n",
    "```\n",
    "\n",
    "### Lexer (Tokenizer)\n",
    "\n",
    "- Reads the raw SRT file text character by character\n",
    "- Identifies meaningful pieces (tokens) like numbers, timestamps, arrows, text\n",
    "- Uses regex patterns to recognize timestamp format: `\\d{2}:\\d{2}:\\d{2},\\d{3}`\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "Input:  \"1\\n00:00:01,000 --> 00:00:03,000\\nHello\\n\\n\"\n",
    "Output: [Token(INDEX, '1'), Token(NEWLINE, '\\n'),\n",
    "         Token(TIMESTAMP, '00:00:01,000'), Token(ARROW, '-->'),\n",
    "         Token(TIMESTAMP, '00:00:03,000'), Token(NEWLINE, '\\n'),\n",
    "         Token(TEXT, 'Hello'), Token(NEWLINE, '\\n'),\n",
    "         Token(BLANK_LINE, ''), Token(EOF, '')]\n",
    "```\n",
    "\n",
    "### Parser\n",
    "\n",
    "- Takes the token stream from the lexer\n",
    "- Checks that tokens appear in the correct order (grammar validation)\n",
    "- Builds SubtitleEntry objects (Abstract Syntax Tree)\n",
    "- Validates semantic rules (e.g., start time < end time)\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "Input:  [Token(INDEX, '1'), Token(NEWLINE), Token(TIMESTAMP, '00:00:01,000'), ...]\n",
    "Output: [SubtitleEntry(index=1, start=00:00:01,000, end=00:00:03,000, text=['Hello'])]\n",
    "```\n",
    "\n",
    "### Executor\n",
    "\n",
    "- Takes validated subtitle entries\n",
    "- Optionally translates text using Google Translate\n",
    "- Displays subtitles with simulated timing (using `time.sleep()`)\n",
    "- Formats output as: `[00:00:01.000] DISPLAY: \"Hello\"`\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "Input:  [SubtitleEntry(1, start, end, ['Hello'])]\n",
    "Output: [00:00:01.000] DISPLAY: \"Hello\"\n",
    "        [00:00:03.000] CLEAR\n",
    "```\n",
    "\n",
    "### Error Handling Strategy\n",
    "\n",
    "We handle errors at each stage:\n",
    "\n",
    "1. Lexer Errors (`LexerError`):\n",
    "\n",
    "   - Malformed timestamps (e.g., `25:99:99,000`)\n",
    "   - Missing arrow in timestamp line\n",
    "   - Invalid characters in timestamps\n",
    "\n",
    "2. Parser Errors (`ParserError`):\n",
    "\n",
    "   - Missing index numbers\n",
    "   - Non-sequential indices (e.g., jumps from 1 to 3)\n",
    "   - Start time after end time\n",
    "   - Missing text content\n",
    "   - Missing blank line separator\n",
    "\n",
    "3. Executor Errors (`ExecutorError`):\n",
    "   - No subtitles to display (empty file)\n",
    "   - Translation library not installed\n",
    "   - Unsupported target language\n",
    "\n",
    "We chose this three-stage pipeline design (lexer, parser, evaluator) with because it allows each component to handle one job on its own, making the system easier to debug, test and understand.\n",
    "\n",
    "We chose Fail-fast error handling because it provides clear feedback and we think it suits SRT files well (a single error usually requires fixing the entire file anyway).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Implementation Details\n",
    "\n",
    "### Setup: Import the Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreter ready!\n"
     ]
    }
   ],
   "source": [
    "# Import the main interpreter class\n",
    "from src.interpreter import SRTInterpreter\n",
    "\n",
    "# Create an interpreter instance\n",
    "interpreter = SRTInterpreter()\n",
    "\n",
    "print(\"Interpreter ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexer (Tokenization)\n",
    "\n",
    "1. Reads the file line by line\n",
    "2. Uses regex patterns to identify timestamps and index numbers\n",
    "3. Categorizes everything as tokens (INDEX, TIMESTAMP, TEXT, etc.)\n",
    "4. Keeps track of line numbers for error reporting\n",
    "\n",
    "Regex patterns:\n",
    "\n",
    "- Timestamp: `^\\d{2}:\\d{2}:\\d{2},\\d{3}$` (matches `00:01:30,500`)\n",
    "- Index: `^\\d+$` (matches any positive integer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens generated by the lexer:\n",
      "==================================================\n",
      "1. INDEX \t | '1' (line 1)\n",
      "2. NEWLINE \t | '\\n' (line 1)\n",
      "3. TIMESTAMP \t | '00:00:01,000' (line 2)\n",
      "4. ARROW \t | '-->' (line 2)\n",
      "5. TIMESTAMP \t | '00:00:03,000' (line 2)\n",
      "6. NEWLINE \t | '\\n' (line 2)\n",
      "7. TEXT \t | 'Hello, world!' (line 3)\n",
      "8. NEWLINE \t | '\\n' (line 3)\n",
      "9. BLANK_LINE \t | '' (line 4)\n",
      "10. BLANK_LINE \t | '' (line 5)\n",
      "11. EOF \t | '' (line 5)\n",
      "\n",
      "Total tokens: 11\n"
     ]
    }
   ],
   "source": [
    "from src.lexer import Lexer\n",
    "\n",
    "# Create a lexer instance\n",
    "lexer = Lexer()\n",
    "\n",
    "# Sample SRT text (one complete subtitle)\n",
    "srt_text = \"\"\"1\n",
    "00:00:01,000 --> 00:00:03,000\n",
    "Hello, world!\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = lexer.tokenize(srt_text)\n",
    "\n",
    "# Display all tokens\n",
    "print(\"Tokens generated by the lexer:\")\n",
    "print(\"=\"*50)\n",
    "for i, token in enumerate(tokens, 1):\n",
    "    print(f\"{i}. {token.type} \\t | {repr(token.value)} (line {token.line_number})\")\n",
    "\n",
    "print(f\"\\nTotal tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "- The lexer identified the index number `1` as an INDEX token\n",
    "- Recognized the two timestamps with the correct format\n",
    "- Found the arrow `-->` between timestamps\n",
    "- Categorized `Hello, world!` as TEXT\n",
    "- Tracked newlines and the blank line separator\n",
    "- Added an EOF (end-of-file) token at the end\n",
    "\n",
    "Error handling example: What if the timestamp is malformed?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.lexer import LexerError\n",
    "\n",
    "# This timestamp has invalid minutes (99 > 59)\n",
    "bad_srt = \"\"\"1\n",
    "00:99:01,000 --> 00:00:03,000\n",
    "Bad timestamp!\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    lexer_test = Lexer()\n",
    "    tokens = lexer_test.tokenize(bad_srt)\n",
    "except LexerError as e:\n",
    "    print(f\"Lexer Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above executes with no errors because the lexer only validates format (regex pattern matching). It still has the correct structure (two digits for each component), but this would fail in the parser stage where semantic validation occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Parser (Building Subtitle Entries)\n",
    "\n",
    "1. Iterates through tokens one by one\n",
    "2. Expects a specific pattern: INDEX → NEWLINE → TIMESTAMP → ARROW → TIMESTAMP → NEWLINE → TEXT → BLANK_LINE\n",
    "3. Validates semantic rules (e.g., start time before end time)\n",
    "4. Creates `SubtitleEntry` objects (our AST nodes)\n",
    "\n",
    "Parsing technique: Recursive descent\n",
    "- Each grammar rule becomes a function\n",
    "- Functions call each other to parse nested structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser found 1 subtitle(s):\n",
      "==================================================\n",
      "\n",
      "Subtitle #1\n",
      "  Start time: 00:00:01,000\n",
      "  End time:   00:00:03,000\n",
      "  Text:       Hello, world!\n",
      "  Duration:   2000ms\n"
     ]
    }
   ],
   "source": [
    "from src.parser import Parser\n",
    "\n",
    "# Use the tokens we generated earlier\n",
    "lexer_demo = Lexer()\n",
    "tokens_demo = lexer_demo.tokenize(srt_text)\n",
    "\n",
    "# Parse the tokens\n",
    "parser = Parser(tokens_demo)\n",
    "entries = parser.parse()\n",
    "\n",
    "# Display the parsed subtitle entries\n",
    "print(f\"Parser found {len(entries)} subtitle(s):\")\n",
    "print(\"=\"*50)\n",
    "for entry in entries:\n",
    "    print(f\"\\nSubtitle #{entry.index}\")\n",
    "    print(f\"  Start time: {entry.start_time}\")\n",
    "    print(f\"  End time:   {entry.end_time}\")\n",
    "    print(f\"  Text:       {entry.get_text()}\")\n",
    "    print(f\"  Duration:   {entry.end_time.to_milliseconds() - entry.start_time.to_milliseconds()}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "- Parser consumed tokens in order\n",
    "- Created a `SubtitleEntry` object with structured data\n",
    "- Validated that start time (1 sec) comes before end time (3 sec)\n",
    "- Stored text as a list of lines (for multiline support)\n",
    "\n",
    "Error handling example: What if we're missing the index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parser Error: Expected subtitle number 1\n"
     ]
    }
   ],
   "source": [
    "from src.parser import ParserError\n",
    "\n",
    "# This SRT is missing the index number\n",
    "bad_srt_no_index = \"\"\"00:00:01,000 --> 00:00:03,000\n",
    "Missing index!\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    lexer_test = Lexer()\n",
    "    tokens_test = lexer_test.tokenize(bad_srt_no_index)\n",
    "    parser_test = Parser(tokens_test)\n",
    "    entries_test = parser_test.parse()\n",
    "except ParserError as e:\n",
    "    print(f\"Parser Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Executor (Display and Translation)\n",
    "\n",
    "1. Optionally translates all subtitle text using Google Translate API\n",
    "2. Iterates through subtitle entries\n",
    "3. Displays each subtitle with its timestamp\n",
    "\n",
    "Translation process:\n",
    "- Uses the `deep-translator` library with `GoogleTranslator`\n",
    "- Translates each line of text individually\n",
    "- Supports 5 languages: English, Filipino (Tagalog), Korean, Chinese, Japanese\n",
    "- Falls back to original text if translation fails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying subtitles (English):\n",
      "==================================================\n",
      "[00:00:01.000] DISPLAY: \"Hello, world!\"\n",
      "[00:00:03.000] CLEAR\n"
     ]
    }
   ],
   "source": [
    "from src.executor import Executor\n",
    "\n",
    "# Use the subtitle entries we parsed earlier\n",
    "executor = Executor()\n",
    "\n",
    "print(\"Displaying subtitles (English):\")\n",
    "print(\"=\"*50)\n",
    "executor.execute(entries, translate_to='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "- Executor displays each subtitle with its start timestamp\n",
    "- Shows \"CLEAR\" at the end timestamp\n",
    "- Uses delays to simulate real subtitle timing\n",
    "- Formats timestamps nicely: `00:00:01.000`\n",
    "\n",
    "Now let's translate to Filipino:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Displaying subtitles (Filipino/Tagalog):\n",
      "==================================================\n",
      "\n",
      "Translating to filipino...\n",
      "Translation complete!                    \n",
      "\n",
      "[00:00:01.000] DISPLAY: \"Kumusta, Mundo!\"\n",
      "[00:00:03.000] CLEAR\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDisplaying subtitles (Filipino/Tagalog):\")\n",
    "print(\"=\"*50)\n",
    "executor_fil = Executor()\n",
    "executor_fil.execute(entries, translate_to='filipino')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "1. Before display, executor calls `translate_subtitles()`\n",
    "2. Maps language name → language code (e.g., `'filipino'` → `'tl'`)\n",
    "3. Creates a `GoogleTranslator` instance with source='en', target='tl'\n",
    "4. Translates each text line using the `translate()` method\n",
    "5. Creates new subtitle entries with translated text\n",
    "6. Displays translated subtitles\n",
    "\n",
    "The translation is done by Google's neural machine translation system, which handles context and grammar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Pipeline Demo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreter Pipeline Demo\n",
      "==================================================\n",
      "\n",
      "[Step 1: Lexer - Tokenization]\n",
      "✓ Generated 29 tokens\n",
      "\n",
      "[Step 2: Parser - Validation & AST Building]\n",
      "✓ Parsed 3 valid subtitle entries\n",
      "\n",
      "[Step 3: Executor - Display]\n",
      "\n",
      "Translating to filipino...\n",
      "Translation complete!                    \n",
      "\n",
      "[00:00:01.000] DISPLAY: \"Maligayang pagdating sa aming tagasalin!\"\n",
      "[00:00:03.000] CLEAR\n",
      "[00:00:04.000] DISPLAY: \"Maaari itong hawakan ang maraming mga subtitle.\"\n",
      "[00:00:06.000] CLEAR\n",
      "[00:00:07.000] DISPLAY: \"At isalin din ang mga ito!\"\n",
      "[00:00:09.000] CLEAR\n"
     ]
    }
   ],
   "source": [
    "# Full pipeline demonstration\n",
    "print(\"Interpreter Pipeline Demo\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Sample SRT with multiple subtitles\n",
    "full_srt = \"\"\"1\n",
    "00:00:01,000 --> 00:00:03,000\n",
    "Welcome to our interpreter!\n",
    "\n",
    "2\n",
    "00:00:04,000 --> 00:00:06,000\n",
    "It can handle multiple subtitles.\n",
    "\n",
    "3\n",
    "00:00:07,000 --> 00:00:09,000\n",
    "And translate them too!\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: Lexer\n",
    "print(\"\\n[Step 1: Lexer - Tokenization]\")\n",
    "lexer_full = Lexer()\n",
    "tokens_full = lexer_full.tokenize(full_srt)\n",
    "print(f\"✓ Generated {len(tokens_full)} tokens\")\n",
    "\n",
    "# Step 2: Parser\n",
    "print(\"\\n[Step 2: Parser - Validation & AST Building]\")\n",
    "parser_full = Parser(tokens_full)\n",
    "entries_full = parser_full.parse()\n",
    "print(f\"✓ Parsed {len(entries_full)} valid subtitle entries\")\n",
    "\n",
    "# Step 3: Executor\n",
    "print(\"\\n[Step 3: Executor - Display]\")\n",
    "executor_full = Executor()\n",
    "executor_full.execute(entries_full, translate_to='filipino')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Testing with Valid and Invalid Inputs\n",
    "\n",
    "### Test 1: Valid Basic Subtitle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: Valid Basic SRT File\n",
      "==================================================\n",
      "File contents:\n",
      "1\n",
      "00:00:01,000 --> 00:00:03,000\n",
      "Hello world!\n",
      "\n",
      "2\n",
      "00:00:04,000 --> 00:00:06,000\n",
      "This is a test.\n",
      "\n",
      "\n",
      "\n",
      "Running interpreter:\n",
      "Reading file: examples/valid_basic.srt\n",
      "\n",
      "Step 1: Tokenizing...\n",
      "  Found 20 tokens\n",
      "\n",
      "Step 2: Parsing...\n",
      "  Found 2 subtitles\n",
      "\n",
      "Step 3: Displaying subtitles\n",
      "\n",
      "[00:00:01.000] DISPLAY: \"Hello world!\"\n",
      "[00:00:03.000] CLEAR\n",
      "[00:00:04.000] DISPLAY: \"This is a test.\"\n",
      "[00:00:06.000] CLEAR\n",
      "\n",
      "Done!\n",
      "Test passed: File processed successfully\n"
     ]
    }
   ],
   "source": [
    "print(\"Test 1: Valid Basic SRT File\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Show the file contents\n",
    "with open('examples/valid_basic.srt', 'r') as f:\n",
    "    content = f.read()\n",
    "    print(\"File contents:\")\n",
    "    print(content)\n",
    "\n",
    "# Run the interpreter\n",
    "print(\"\\nRunning interpreter:\")\n",
    "interpreter.run('examples/valid_basic.srt', 'english')\n",
    "\n",
    "print(\"Test passed: File processed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Valid Multiline Subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 2: Valid Multiline Subtitles\n",
      "==================================================\n",
      "File contents:\n",
      "1\n",
      "00:00:01,000 --> 00:00:04,000\n",
      "This subtitle has\n",
      "multiple lines\n",
      "of text.\n",
      "\n",
      "\n",
      "\n",
      "Running interpreter:\n",
      "Reading file: examples/valid_multiline.srt\n",
      "\n",
      "Step 1: Tokenizing...\n",
      "  Found 15 tokens\n",
      "\n",
      "Step 2: Parsing...\n",
      "  Found 1 subtitles\n",
      "\n",
      "Step 3: Displaying subtitles\n",
      "\n",
      "[00:00:01.000] DISPLAY: \"This subtitle has\n",
      "multiple lines\n",
      "of text.\"\n",
      "[00:00:04.000] CLEAR\n",
      "\n",
      "Done!\n",
      "Test passed: Multiline text handled correctly\n"
     ]
    }
   ],
   "source": [
    "print(\"Test 2: Valid Multiline Subtitles\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "with open('examples/valid_multiline.srt', 'r') as f:\n",
    "    content = f.read()\n",
    "    print(\"File contents:\")\n",
    "    print(content)\n",
    "\n",
    "print(\"\\nRunning interpreter:\")\n",
    "interpreter.run('examples/valid_multiline.srt', 'english')\n",
    "\n",
    "print(\"Test passed: Multiline text handled correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3: Translation to Korean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 3: Translation Feature (Korean)\n",
      "==================================================\n",
      "Original (English):\n",
      "Reading file: examples/valid_basic.srt\n",
      "\n",
      "Step 1: Tokenizing...\n",
      "  Found 20 tokens\n",
      "\n",
      "Step 2: Parsing...\n",
      "  Found 2 subtitles\n",
      "\n",
      "Step 3: Displaying subtitles\n",
      "\n",
      "[00:00:01.000] DISPLAY: \"Hello world!\"\n",
      "[00:00:03.000] CLEAR\n",
      "[00:00:04.000] DISPLAY: \"This is a test.\"\n",
      "[00:00:06.000] CLEAR\n",
      "\n",
      "Done!\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Translated (Korean):\n",
      "Reading file: examples/valid_basic.srt\n",
      "\n",
      "Step 1: Tokenizing...\n",
      "  Found 20 tokens\n",
      "\n",
      "Step 2: Parsing...\n",
      "  Found 2 subtitles\n",
      "\n",
      "Step 3: Displaying subtitles\n",
      "  (will translate to korean)\n",
      "\n",
      "\n",
      "Translating to korean...\n",
      "Translation complete!                    \n",
      "\n",
      "[00:00:01.000] DISPLAY: \"안녕하세요!\"\n",
      "[00:00:03.000] CLEAR\n",
      "[00:00:04.000] DISPLAY: \"이것은 테스트입니다.\"\n",
      "[00:00:06.000] CLEAR\n",
      "\n",
      "Done!\n",
      "Test passed: Translation works correctly\n"
     ]
    }
   ],
   "source": [
    "print(\"Test 3: Translation Feature (Korean)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"Original (English):\")\n",
    "interpreter.run('examples/valid_basic.srt', 'english')\n",
    "\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Translated (Korean):\")\n",
    "interpreter.run('examples/valid_basic.srt', 'korean')\n",
    "\n",
    "print(\"Test passed: Translation works correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4: Invalid Input - Missing Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 4: Invalid Input - Missing Index Number\n",
      "==================================================\n",
      "File contents (INVALID):\n",
      "1\n",
      "00:00:01,000 --> 00:00:03,000\n",
      "First subtitle is fine.\n",
      "\n",
      "00:00:04,000 --> 00:00:06,000\n",
      "This subtitle is missing its index!\n",
      "\n",
      "\n",
      "\n",
      "Running interpreter (expecting error):\n",
      "Reading file: examples/invalid_missing_index.srt\n",
      "\n",
      "Step 1: Tokenizing...\n",
      "  Found 18 tokens\n",
      "\n",
      "Step 2: Parsing...\n",
      "Parser Error: Expected subtitle number 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Test 4: Invalid Input - Missing Index Number\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "with open('examples/invalid_missing_index.srt', 'r') as f:\n",
    "    content = f.read()\n",
    "    print(\"File contents (INVALID):\")\n",
    "    print(content)\n",
    "\n",
    "print(\"\\nRunning interpreter (expecting error):\")\n",
    "try:\n",
    "    interpreter.run('examples/invalid_missing_index.srt', 'english')\n",
    "except Exception as e:\n",
    "    print(f\"Error caught: {e}\")\n",
    "    print(\"Test passed: Parser correctly detected missing index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 5: Invalid Input - Bad Timestamp Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 5: Invalid Input - Bad Timestamp Order\n",
      "==================================================\n",
      "File contents (INVALID - end before start):\n",
      "1\n",
      "00:00:01,000 --> 00:00:03,000\n",
      "First subtitle is fine.\n",
      "\n",
      "2\n",
      "00:00:08,000 --> 00:00:05,000\n",
      "This subtitle has start time AFTER end time!\n",
      "\n",
      "\n",
      "\n",
      "Running interpreter (expecting error):\n",
      "Reading file: examples/invalid_timestamp_order.srt\n",
      "\n",
      "Step 1: Tokenizing...\n",
      "  Found 20 tokens\n",
      "\n",
      "Step 2: Parsing...\n",
      "Parser Error: Start time 00:00:08,000 must be before end time 00:00:05,000\n"
     ]
    }
   ],
   "source": [
    "print(\"Test 5: Invalid Input - Bad Timestamp Order\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "with open('examples/invalid_timestamp_order.srt', 'r') as f:\n",
    "    content = f.read()\n",
    "    print(\"File contents (INVALID - end before start):\")\n",
    "    print(content)\n",
    "\n",
    "print(\"\\nRunning interpreter (expecting error):\")\n",
    "try:\n",
    "    interpreter.run('examples/invalid_timestamp_order.srt', 'english')\n",
    "except Exception as e:\n",
    "    print(f\"Error caught: {e}\")\n",
    "    print(\"Test passed: Parser correctly detected temporal violation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 6: Invalid Input - Malformed Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 6: Invalid Input - Malformed Timestamp\n",
      "==================================================\n",
      "File contents (INVALID - bad time format):\n",
      "1\n",
      "00:00:01,000 --> 00:00:03,000\n",
      "First subtitle is fine.\n",
      "\n",
      "2\n",
      "00:00:04,000 --> 00:99:99,999\n",
      "This subtitle has invalid timestamp ranges (minutes and seconds > 59)!\n",
      "\n",
      "\n",
      "\n",
      "Running interpreter (expecting error):\n",
      "Reading file: examples/invalid_malformed_time.srt\n",
      "\n",
      "Step 1: Tokenizing...\n",
      "  Found 20 tokens\n",
      "\n",
      "Step 2: Parsing...\n",
      "Parser Error: Bad end time: Invalid time: 00:99:99,999\n"
     ]
    }
   ],
   "source": [
    "print(\"Test 6: Invalid Input - Malformed Timestamp\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "with open('examples/invalid_malformed_time.srt', 'r') as f:\n",
    "    content = f.read()\n",
    "    print(\"File contents (INVALID - bad time format):\")\n",
    "    print(content)\n",
    "\n",
    "print(\"\\nRunning interpreter (expecting error):\")\n",
    "try:\n",
    "    interpreter.run('examples/invalid_malformed_time.srt', 'english')\n",
    "except Exception as e:\n",
    "    print(f\"Error caught: {e}\")\n",
    "    print(\"Test passed: Lexer correctly detected malformed timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 7: Complex Real-World File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 7: Complex Real-World SRT File\n",
      "==================================================\n",
      "File preview (first 400 characters):\n",
      "1\n",
      "00:00:01,000 --> 00:00:02,500\n",
      "Welcome to the film festival.\n",
      "\n",
      "2\n",
      "00:00:03,500 --> 00:00:07,000\n",
      "The opening ceremony begins shortly.\n",
      "\n",
      "3\n",
      "00:00:08,000 --> 00:00:18,000\n",
      "Please silence your mobile phones and enjoy the experience.\n",
      "\n",
      "4\n",
      "00:00:19,000 --> 00:00:21,500\n",
      "Act One\n",
      "\n",
      "5\n",
      "00:00:23,000 --> 00:00:28,000\n",
      "The story begins on a quiet evening in a small coastal town.\n",
      "\n",
      "6\n",
      "00:00:29,500 --> 00:00:35,000\n",
      "The pro\n",
      "...\n",
      "\n",
      "Running interpreter:\n",
      "Reading file: examples/valid_complex.srt\n",
      "\n",
      "Step 1: Tokenizing...\n",
      "  Found 202 tokens\n",
      "\n",
      "Step 2: Parsing...\n",
      "  Found 20 subtitles\n",
      "\n",
      "Step 3: Displaying subtitles\n",
      "\n",
      "[00:00:01.000] DISPLAY: \"Welcome to the film festival.\"\n",
      "[00:00:02.500] CLEAR\n",
      "[00:00:03.500] DISPLAY: \"The opening ceremony begins shortly.\"\n",
      "[00:00:07.000] CLEAR\n",
      "[00:00:08.000] DISPLAY: \"Please silence your mobile phones and enjoy the experience.\"\n",
      "[00:00:18.000] CLEAR\n",
      "[00:00:19.000] DISPLAY: \"Act One\"\n",
      "[00:00:21.500] CLEAR\n",
      "[00:00:23.000] DISPLAY: \"The story begins on a quiet evening in a small coastal town.\"\n",
      "[00:00:28.000] CLEAR\n",
      "[00:00:29.500] DISPLAY: \"The protagonist had been waiting for this moment for years, planning every detail meticulously, knowing that success would depend on precise timing and unwavering determination.\"\n",
      "[00:00:35.000] CLEAR\n",
      "[00:00:36.500] DISPLAY: \"She walked through the empty street.\n",
      "The moon cast long shadows.\"\n",
      "[00:00:40.000] CLEAR\n",
      "[00:00:41.500] DISPLAY: \"Memories flooded back.\n",
      "Each step reminded her of the past.\n",
      "Nothing felt the same anymore.\"\n",
      "[00:00:46.000] CLEAR\n",
      "[00:00:48.000] DISPLAY: \"The detective examined the evidence carefully.\n",
      "Every clue mattered.\n",
      "Time was running out.\n",
      "The truth had to be uncovered.\"\n",
      "[00:00:53.000] CLEAR\n",
      "[00:00:55.000] DISPLAY: \"<i>This note was written yesterday.</i>\"\n",
      "[00:00:58.500] CLEAR\n",
      "[00:01:00.000] DISPLAY: \"<b>The suspect was seen at midnight.</b>\"\n",
      "[00:01:03.000] CLEAR\n",
      "[00:01:04.500] DISPLAY: \"<u>Location: Harbor District</u>\"\n",
      "[00:01:07.500] CLEAR\n",
      "[00:01:09.000] DISPLAY: \"<font color=\"#00FF00\">Security footage confirmed the timeline.</font>\"\n",
      "[00:01:12.500] CLEAR\n",
      "[00:01:14.000] DISPLAY: \"<font color=\"#0000FF\">Two witnesses</font> provided <font color=\"#FFFF00\">conflicting accounts</font> of the incident.\"\n",
      "[00:01:18.000] CLEAR\n",
      "[00:01:20.000] DISPLAY: \"<i><b>The evidence pointed to an inside job.</b></i>\"\n",
      "[00:01:24.000] CLEAR\n",
      "[00:01:25.500] DISPLAY: \"<b><u>Critical discovery: The safe was opened from within.</u></b>\"\n",
      "[00:01:29.000] CLEAR\n",
      "[00:01:31.000] DISPLAY: \"<i><font color=\"#FF00FF\">Trust no one in this investigation.</font></i>\"\n",
      "[00:01:35.500] CLEAR\n",
      "[00:01:37.000] DISPLAY: \"The captain reviewed <b>all the reports</b> carefully.\n",
      "She noticed <i>small inconsistencies</i> in the statements.\n",
      "<u>Something was clearly wrong.</u>\"\n",
      "[00:01:42.000] CLEAR\n",
      "[00:01:44.000] DISPLAY: \"<font color=\"#FF0000\">Breaking news from headquarters</font>: The investigation had uncovered a conspiracy involving multiple departments, and the truth was far more complex than anyone had initially imagined.\"\n",
      "[00:01:50.500] CLEAR\n",
      "[00:01:52.000] DISPLAY: \"<i>To be continued...</i>\n",
      "<b><font color=\"#FFD700\">The truth will be revealed</font></b> in the next episode.\n",
      "Stay tuned for more shocking discoveries.\"\n",
      "[00:01:58.000] CLEAR\n",
      "\n",
      "Done!\n",
      "Test passed: Complex file with multiple subtitles processed successfully\n"
     ]
    }
   ],
   "source": [
    "print(\"Test 7: Complex Real-World SRT File\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Show first few subtitles\n",
    "with open('examples/valid_complex.srt', 'r') as f:\n",
    "    content = f.read()\n",
    "    print(\"File preview (first 400 characters):\")\n",
    "    print(content[:400])\n",
    "    print(\"...\\n\")\n",
    "\n",
    "print(\"Running interpreter:\")\n",
    "interpreter.run('examples/valid_complex.srt', 'english')\n",
    "\n",
    "print(\"Test passed: Complex file with multiple subtitles processed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Extensions and Additional Features\n",
    "\n",
    "### Multi-Language Translation\n",
    "\n",
    "What it does:\n",
    "- Translates subtitle text from English to multiple target languages\n",
    "- Uses Google Translate API via the `deep-translator` library\n",
    "- Maintains original timing and structure\n",
    "\n",
    "Supported languages:\n",
    "1. English (original/default)\n",
    "2. Filipino (Tagalog) - `'filipino'` or `'tagalog'`\n",
    "3. Korean - `'korean'`\n",
    "4. Chinese (Simplified) - `'chinese'`\n",
    "5. Japanese - `'japanese'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation Extension Demo\n",
      "==================================================\n",
      "\n",
      "1. Original (English):\n",
      "Reading file: examples/valid_basic.srt\n",
      "\n",
      "Step 1: Tokenizing...\n",
      "  Found 20 tokens\n",
      "\n",
      "Step 2: Parsing...\n",
      "  Found 2 subtitles\n",
      "\n",
      "Step 3: Displaying subtitles\n",
      "\n",
      "[00:00:01.000] DISPLAY: \"Hello world!\"\n",
      "[00:00:03.000] CLEAR\n",
      "[00:00:04.000] DISPLAY: \"This is a test.\"\n",
      "[00:00:06.000] CLEAR\n",
      "\n",
      "Done!\n",
      "\n",
      "2. Filipino (Tagalog):\n",
      "Reading file: examples/valid_basic.srt\n",
      "\n",
      "Step 1: Tokenizing...\n",
      "  Found 20 tokens\n",
      "\n",
      "Step 2: Parsing...\n",
      "  Found 2 subtitles\n",
      "\n",
      "Step 3: Displaying subtitles\n",
      "  (will translate to filipino)\n",
      "\n",
      "\n",
      "Translating to filipino...\n",
      "Translation complete!                    \n",
      "\n",
      "[00:00:01.000] DISPLAY: \"Hello World!\"\n",
      "[00:00:03.000] CLEAR\n",
      "[00:00:04.000] DISPLAY: \"Ito ay isang pagsubok.\"\n",
      "[00:00:06.000] CLEAR\n",
      "\n",
      "Done!\n",
      "\n",
      "3. Korean:\n",
      "Reading file: examples/valid_basic.srt\n",
      "\n",
      "Step 1: Tokenizing...\n",
      "  Found 20 tokens\n",
      "\n",
      "Step 2: Parsing...\n",
      "  Found 2 subtitles\n",
      "\n",
      "Step 3: Displaying subtitles\n",
      "  (will translate to korean)\n",
      "\n",
      "\n",
      "Translating to korean...\n",
      "Translation complete!                    \n",
      "\n",
      "[00:00:01.000] DISPLAY: \"안녕하세요!\"\n",
      "[00:00:03.000] CLEAR\n",
      "[00:00:04.000] DISPLAY: \"이것은 테스트입니다.\"\n",
      "[00:00:06.000] CLEAR\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Translation Extension Demo\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Original English\n",
    "print(\"\\n1. Original (English):\")\n",
    "interpreter.run('examples/valid_basic.srt', 'english')\n",
    "\n",
    "# Filipino\n",
    "print(\"\\n2. Filipino (Tagalog):\")\n",
    "interpreter.run('examples/valid_basic.srt', 'filipino')\n",
    "\n",
    "# Korean\n",
    "print(\"\\n3. Korean:\")\n",
    "interpreter.run('examples/valid_basic.srt', 'korean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This extension helps people watch videos in different languages, which is useful for language learners who want to see translations side by side and content creators trying to reach wider audiences. Major streaming platforms like Netflix already use similar technology.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Insights and Conclusions\n",
    "\n",
    "Building this SRT subtitle interpreter provided us with practical insight into how language processing actually works. The three-stage architecture (lexer, parser, executor) made the code easier to test and maintain. We spent a bit more time on error handling, learning that clear error messages with line numbers matter just as much as the core functionality. Working with SRT's simple format showed us why minimalist designs like JSON and CSV remain popular: they're easy to parse, debug, and implement across different systems.\n",
    "\n",
    "The interpreter handles real subtitle files effectively, but we think there's still room for growth. Currently, it only displays subtitles sequentially without video synchronization or subtitle editing capabilities. Future versions could integrate with video players, support format conversion between SRT and other subtitle types, or use speech recognition APIs to auto-generate subtitles (imagine that!). Despite these limitations, this machine project gave us an understanding of how interpreters function. \n",
    "\n",
    "Sometimes the best way to understand a complex system is to build a simpler version yourself.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: References\n",
    "\n",
    "### Scholarly Articles and Academic Papers\n",
    "\n",
    "Baicoianu, A., & Plajer, I. (2023). Considerations on efficient lexical analysis in the context of compiler design. Bulletin of the Transilvania University of Brasov. Series III: Mathematics and Computer Science, 159–168. https://doi.org/10.31926/but.mif.2023.3.65.2.14\n",
    "\n",
    "- This paper provided guidelines for constructing efficient lexers (scanners). We applied their recommendations on pattern matching and tokenization to design our Lexer class. The paper's discussion on combining lexers with parsers helped us understand how the two components should interact and pass data between stages.\n",
    "\n",
    "Jordan, W., Bejo, A., & Persada, A. G. (2019). The Development of Lexer and Parser as Parts of Compiler for GAMA32 Processor’s Instruction-set using Python. 2019 International Seminar on Research of Information Technology and Intelligent Systems (ISRITI), 450–455. https://doi.org/10.1109/ISRITI48646.2019.9034617\n",
    "\n",
    "- This paper demonstrated practical lexer and parser implementation in Python. We learned how to structure our tokenization logic and how to organize token types as constants. Their approach to error handling in the parser stage influenced our ParserError design.\n",
    "\n",
    "Pai T, V., Jayanthila Devi, A., & Aithal, P. S. (2020). A Systematic Literature Review of Lexical Analyzer Implementation Techniques in Compiler Design. International Journal of Applied Engineering and Management Letters, 285–301. https://doi.org/10.47992/IJAEML.2581.7000.0087\n",
    "\n",
    "- This literature review helped us compare different approaches to lexical analysis. We chose the regex-based approach (using Python's `re` module) based on their discussion of implementation techniques and trade-offs for simple grammars like SRT.\n",
    "\n",
    "### Online References, Tutorials, and Documentation\n",
    "\n",
    "SubRip. (2025). In Wikipedia. https://en.wikipedia.org/w/index.php?title=SubRip&oldid=1311983151\n",
    "\n",
    "- Primary source for understanding the SRT file format specification. We learned the exact structure of subtitle entries, timestamp format requirements, and common use cases. This defined our grammar rules and validation requirements.\n",
    "\n",
    "SubRip Subtitle format (SRT). (2023, March 28). [Web page]. https://www.loc.gov/preservation/digital/formats/fdd/fdd000569.shtml\n",
    "\n",
    "- Official archival documentation that clarified edge cases and format details. We used this to understand why certain validation rules exist (like requiring sequential indices and proper timestamp formatting).\n",
    "\n",
    "Lexical analysis. (2025). In Wikipedia. https://en.wikipedia.org/w/index.php?title=Lexical_analysis&oldid=1309109190\n",
    "\n",
    "- General background on lexical analysis concepts. Helped us understand the two-stage process (scanning and evaluating) and the difference between lexemes and tokens. We applied these concepts in our Lexer class design.\n",
    "\n",
    "Token, Patterns, and Lexemes. (00:00:34+00:00). GeeksforGeeks. https://www.geeksforgeeks.org/compiler-design/token-patterns-and-lexems/\n",
    "\n",
    "- Clarified the distinction between tokens (categories), patterns (regex), and lexemes (actual text). This helped us design our token type constants (TOKEN_INDEX, TOKEN_TIMESTAMP, etc.) and corresponding regex patterns.\n",
    "\n",
    "deep-translator: A flexible free and unlimited python tool to translate between different languages in a simple way using multiple translators (Version 1.11.4). (n.d.). [Python; OS Independent]. Retrieved October 28, 2025, from https://github.com/nidhaloff/deep_translator\n",
    "\n",
    "- API documentation for the translation library we used in the executor. We learned how to use the GoogleTranslator class, handle language codes, and implement error handling for translation failures.\n",
    "\n",
    "### Artificial Intelligence (AI) Tools\n",
    "\n",
    "1. [Perplexity.ai](https://www.perplexity.ai/search/how-should-i-structure-a-simpl-S65YcLmbTyCpIHwIUhnRNw#8)\n",
    "   - Project structuring and general knowledge: Asked for advice on how to organize the interpreter into separate modules (lexer.py, parser.py, executor.py, ast.py)\n",
    "   - Error handling strategies: Asked for best practices on raising and catching custom exceptions (LexerError, ParserError, ExecutorError)\n",
    "   - Documentation help: Asked for suggestions on writing clear docstrings and comments\n",
    "\n",
    "2. [Claude Code](https://www.claude.com/product/claude-code)\n",
    "   - Python syntax help: Consulted for Python-specific syntax when implementing regex patterns, class structures, and exception handling\n",
    "   - Code review: Requested feedback on whether our design followed standard interpreter architecture patterns\n",
    "\n",
    "### Tools and Libraries\n",
    "\n",
    "Python Standard Library:\n",
    "\n",
    "- `re` (regular expressions) - Built-in module for pattern matching\n",
    "- `time` - Built-in module for timing delays\n",
    "\n",
    "Third-Party Libraries:\n",
    "\n",
    "- `deep-translator` - Google Translate API wrapper for subtitle translation\n",
    "\n",
    "Development Tools:\n",
    "\n",
    "- Python 3.x\n",
    "- Jupyter Notebook (for this demo)\n",
    "- Git (version control)\n",
    "\n",
    "All references were used ethically and in accordance with academic integrity guidelines. We cited sources where ideas came from, used AI as a learning aid (not a code generator), and implemented all functionality ourselves to ensure we understood the concepts.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSS125L_machine_project (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
